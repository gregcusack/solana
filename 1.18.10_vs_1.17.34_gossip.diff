diff --git a/gossip/Cargo.toml b/gossip/Cargo.toml
index 3696342ae8..f9870ac1ee 100644
--- a/gossip/Cargo.toml
+++ b/gossip/Cargo.toml
@@ -55,6 +55,7 @@ thiserror = { workspace = true }
 [dev-dependencies]
 num_cpus = { workspace = true }
 serial_test = { workspace = true }
+solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
 test-case = { workspace = true }
 
 [build-dependencies]
diff --git a/gossip/src/cluster_info.rs b/gossip/src/cluster_info.rs
index 4e4193a19f..56b75010f9 100644
--- a/gossip/src/cluster_info.rs
+++ b/gossip/src/cluster_info.rs
@@ -32,14 +32,14 @@ use {
             CrdsFilter, CrdsTimeouts, ProcessPullStats, CRDS_GOSSIP_PULL_CRDS_TIMEOUT_MS,
         },
         crds_value::{
-            self, AccountsHashes, CrdsData, CrdsValue, CrdsValueLabel, EpochSlotsIndex,
-            LegacySnapshotHashes, LowestSlot, NodeInstance, SnapshotHashes, Version, Vote,
-            MAX_WALLCLOCK,
+            self, AccountsHashes, CrdsData, CrdsValue, CrdsValueLabel, EpochSlotsIndex, LowestSlot,
+            NodeInstance, SnapshotHashes, Version, Vote, MAX_WALLCLOCK,
         },
         duplicate_shred::DuplicateShred,
         epoch_slots::EpochSlots,
         gossip_error::GossipError,
         ping_pong::{self, PingCache, Pong},
+        restart_crds_values::{RestartLastVotedForkSlots, RestartLastVotedForkSlotsError},
         socketaddr, socketaddr_any,
         weighted_shuffle::WeightedShuffle,
     },
@@ -81,7 +81,7 @@ use {
     solana_vote_program::vote_state::MAX_LOCKOUT_HISTORY,
     std::{
         borrow::Cow,
-        collections::{hash_map::Entry, HashMap, HashSet, VecDeque},
+        collections::{HashMap, HashSet, VecDeque},
         fmt::Debug,
         fs::{self, File},
         io::BufReader,
@@ -118,10 +118,6 @@ pub(crate) const DUPLICATE_SHRED_MAX_PAYLOAD_SIZE: usize = PACKET_DATA_SIZE - 11
 /// such that the serialized size of the push/pull message stays below
 /// PACKET_DATA_SIZE.
 pub const MAX_ACCOUNTS_HASHES: usize = 16;
-/// Maximum number of hashes in LegacySnapshotHashes a node publishes
-/// such that the serialized size of the push/pull message stays below
-/// PACKET_DATA_SIZE.
-pub const MAX_LEGACY_SNAPSHOT_HASHES: usize = 16;
 /// Maximum number of incremental hashes in SnapshotHashes a node publishes
 /// such that the serialized size of the push/pull message stays below
 /// PACKET_DATA_SIZE.
@@ -272,7 +268,7 @@ pub fn make_accounts_hashes_message(
 pub(crate) type Ping = ping_pong::Ping<[u8; GOSSIP_PING_TOKEN_SIZE]>;
 
 // TODO These messages should go through the gpu pipeline for spam filtering
-#[frozen_abi(digest = "EnbW8mYTsPMndq9NkHLTkHJgduXvWSfSD6bBdmqQ8TiF")]
+#[frozen_abi(digest = "ogEqvffeEkPpojAaSiUbCv2HdJcdXDQ1ykgYyvKvLo2")]
 #[derive(Serialize, Deserialize, Debug, AbiEnumVisitor, AbiExample)]
 #[allow(clippy::large_enum_variant)]
 pub(crate) enum Protocol {
@@ -398,7 +394,9 @@ fn retain_staked(values: &mut Vec<CrdsValue>, stakes: &HashMap<Pubkey, u64>) {
             CrdsData::AccountsHashes(_) => true,
             CrdsData::LowestSlot(_, _)
             | CrdsData::LegacyVersion(_)
-            | CrdsData::DuplicateShred(_, _) => {
+            | CrdsData::DuplicateShred(_, _)
+            | CrdsData::RestartHeaviestFork(_)
+            | CrdsData::RestartLastVotedForkSlots(_) => {
                 let stake = stakes.get(&value.pubkey()).copied();
                 stake.unwrap_or_default() >= MIN_STAKE_FOR_GOSSIP
             }
@@ -893,10 +891,7 @@ impl ClusterInfo {
                 CrdsData::LowestSlot(0, LowestSlot::new(self_pubkey, min, now)),
                 &self.keypair(),
             );
-            self.local_message_pending_push_queue
-                .lock()
-                .unwrap()
-                .push(entry);
+            self.push_message(entry);
         }
     }
 
@@ -969,6 +964,26 @@ impl ClusterInfo {
         }
     }
 
+    pub fn push_restart_last_voted_fork_slots(
+        &self,
+        fork: &[Slot],
+        last_vote_bankhash: Hash,
+    ) -> Result<(), RestartLastVotedForkSlotsError> {
+        let now = timestamp();
+        let last_voted_fork_slots = RestartLastVotedForkSlots::new(
+            self.id(),
+            now,
+            fork,
+            last_vote_bankhash,
+            self.my_shred_version(),
+        )?;
+        self.push_message(CrdsValue::new_signed(
+            CrdsData::RestartLastVotedForkSlots(last_voted_fork_slots),
+            &self.keypair(),
+        ));
+        Ok(())
+    }
+
     fn time_gossip_read_lock<'a>(
         &'a self,
         label: &'static str,
@@ -977,7 +992,7 @@ impl ClusterInfo {
         TimedGuard::new(self.gossip.crds.read().unwrap(), label, counter)
     }
 
-    pub fn push_message(&self, message: CrdsValue) {
+    fn push_message(&self, message: CrdsValue) {
         self.local_message_pending_push_queue
             .lock()
             .unwrap()
@@ -997,20 +1012,6 @@ impl ClusterInfo {
         self.push_message(CrdsValue::new_signed(message, &self.keypair()));
     }
 
-    pub fn push_legacy_snapshot_hashes(&self, snapshot_hashes: Vec<(Slot, Hash)>) {
-        if snapshot_hashes.len() > MAX_LEGACY_SNAPSHOT_HASHES {
-            warn!(
-                "snapshot hashes too large, ignored: {}",
-                snapshot_hashes.len(),
-            );
-            return;
-        }
-
-        let message =
-            CrdsData::LegacySnapshotHashes(LegacySnapshotHashes::new(self.id(), snapshot_hashes));
-        self.push_message(CrdsValue::new_signed(message, &self.keypair()));
-    }
-
     pub fn push_snapshot_hashes(
         &self,
         full: (Slot, Hash),
@@ -1208,15 +1209,6 @@ impl ClusterInfo {
             .map(map)
     }
 
-    pub fn get_legacy_snapshot_hash_for_node<F, Y>(&self, pubkey: &Pubkey, map: F) -> Option<Y>
-    where
-        F: FnOnce(&Vec<(Slot, Hash)>) -> Y,
-    {
-        let gossip_crds = self.gossip.crds.read().unwrap();
-        let hashes = &gossip_crds.get::<&LegacySnapshotHashes>(*pubkey)?.hashes;
-        Some(map(hashes))
-    }
-
     pub fn get_snapshot_hashes_for_node(&self, pubkey: &Pubkey) -> Option<SnapshotHashes> {
         self.gossip
             .crds
@@ -1227,7 +1219,7 @@ impl ClusterInfo {
     }
 
     /// Returns epoch-slots inserted since the given cursor.
-    /// Excludes entries from nodes with unkown or different shred version.
+    /// Excludes entries from nodes with unknown or different shred version.
     pub fn get_epoch_slots(&self, cursor: &mut Cursor) -> Vec<EpochSlots> {
         let self_shred_version = Some(self.my_shred_version());
         let gossip_crds = self.gossip.crds.read().unwrap();
@@ -1244,6 +1236,24 @@ impl ClusterInfo {
             .collect()
     }
 
+    pub fn get_restart_last_voted_fork_slots(
+        &self,
+        cursor: &mut Cursor,
+    ) -> Vec<RestartLastVotedForkSlots> {
+        let self_shred_version = self.my_shred_version();
+        let gossip_crds = self.gossip.crds.read().unwrap();
+        gossip_crds
+            .get_entries(cursor)
+            .filter_map(|entry| {
+                let CrdsData::RestartLastVotedForkSlots(slots) = &entry.value.data else {
+                    return None;
+                };
+                (slots.shred_version == self_shred_version).then_some(slots)
+            })
+            .cloned()
+            .collect()
+    }
+
     /// Returns duplicate-shreds inserted since the given cursor.
     pub(crate) fn get_duplicate_shreds(&self, cursor: &mut Cursor) -> Vec<DuplicateShred> {
         let gossip_crds = self.gossip.crds.read().unwrap();
@@ -1542,25 +1552,23 @@ impl ClusterInfo {
         (pings, pulls.collect())
     }
 
-    fn drain_push_queue(&self) -> Vec<CrdsValue> {
-        let mut push_queue = self.local_message_pending_push_queue.lock().unwrap();
-        std::mem::take(&mut *push_queue)
-    }
-    // Used in tests
     pub fn flush_push_queue(&self) {
-        let pending_push_messages = self.drain_push_queue();
-        let mut gossip_crds = self.gossip.crds.write().unwrap();
-        let now = timestamp();
-        for entry in pending_push_messages {
-            let _ = gossip_crds.insert(entry, now, GossipRoute::LocalMessage);
+        let entries: Vec<CrdsValue> =
+            std::mem::take(&mut *self.local_message_pending_push_queue.lock().unwrap());
+        if !entries.is_empty() {
+            let mut gossip_crds = self.gossip.crds.write().unwrap();
+            let now = timestamp();
+            for entry in entries {
+                let _ = gossip_crds.insert(entry, now, GossipRoute::LocalMessage);
+            }
         }
     }
     fn new_push_requests(&self, stakes: &HashMap<Pubkey, u64>) -> Vec<(SocketAddr, Protocol)> {
         let self_id = self.id();
         let (mut push_messages, num_entries, num_nodes) = {
             let _st = ScopedTimer::from(&self.stats.new_push_requests);
-            self.gossip
-                .new_push_messages(&self_id, self.drain_push_queue(), timestamp(), stakes)
+            self.flush_push_queue();
+            self.gossip.new_push_messages(&self_id, timestamp(), stakes)
         };
         self.stats
             .push_fanout_num_entries
@@ -1745,7 +1753,7 @@ impl ClusterInfo {
         match gossip_crds.trim(cap, &keep, stakes, timestamp()) {
             Err(err) => {
                 self.stats.trim_crds_table_failed.add_relaxed(1);
-                // TODO: Stakes are comming from the root-bank. Debug why/when
+                // TODO: Stakes are coming from the root-bank. Debug why/when
                 // they are empty/zero.
                 debug!("crds table trim failed: {:?}", err);
             }
@@ -2128,77 +2136,27 @@ impl ClusterInfo {
 
     fn handle_batch_pull_responses(
         &self,
-        responses: Vec<(Pubkey, Vec<CrdsValue>)>,
-        thread_pool: &ThreadPool,
+        responses: Vec<CrdsValue>,
         stakes: &HashMap<Pubkey, u64>,
         epoch_duration: Duration,
     ) {
         let _st = ScopedTimer::from(&self.stats.handle_batch_pull_responses_time);
-        if responses.is_empty() {
-            return;
-        }
-        fn extend<K, V>(hash_map: &mut HashMap<K, Vec<V>>, (key, mut value): (K, Vec<V>))
-        where
-            K: Eq + std::hash::Hash,
-        {
-            match hash_map.entry(key) {
-                Entry::Occupied(mut entry) => {
-                    let entry_value = entry.get_mut();
-                    if entry_value.len() < value.len() {
-                        std::mem::swap(entry_value, &mut value);
-                    }
-                    entry_value.extend(value);
-                }
-                Entry::Vacant(entry) => {
-                    entry.insert(value);
-                }
-            }
-        }
-        fn merge<K, V>(
-            mut hash_map: HashMap<K, Vec<V>>,
-            other: HashMap<K, Vec<V>>,
-        ) -> HashMap<K, Vec<V>>
-        where
-            K: Eq + std::hash::Hash,
-        {
-            if hash_map.len() < other.len() {
-                return merge(other, hash_map);
-            }
-            for kv in other {
-                extend(&mut hash_map, kv);
-            }
-            hash_map
-        }
-        let responses = thread_pool.install(|| {
-            responses
-                .into_par_iter()
-                .with_min_len(1024)
-                .fold(HashMap::new, |mut hash_map, kv| {
-                    extend(&mut hash_map, kv);
-                    hash_map
-                })
-                .reduce(HashMap::new, merge)
-        });
         if !responses.is_empty() {
             let self_pubkey = self.id();
             let timeouts = self
                 .gossip
                 .make_timeouts(self_pubkey, stakes, epoch_duration);
-            for (from, data) in responses {
-                self.handle_pull_response(&from, data, &timeouts);
-            }
+            self.handle_pull_response(responses, &timeouts);
         }
     }
 
     // Returns (failed, timeout, success)
     fn handle_pull_response(
         &self,
-        from: &Pubkey,
         crds_values: Vec<CrdsValue>,
         timeouts: &CrdsTimeouts,
     ) -> (usize, usize, usize) {
         let len = crds_values.len();
-        trace!("PullResponse me: {} from: {} len={}", self.id(), from, len);
         let mut pull_stats = ProcessPullStats::default();
         let (filtered_pulls, filtered_pulls_expired_timeout, failed_inserts) = {
             let _st = ScopedTimer::from(&self.stats.filter_pull_response);
@@ -2473,9 +2431,9 @@ impl ClusterInfo {
                 Protocol::PullRequest(filter, caller) => {
                     pull_requests.push((from_addr, filter, caller))
                 }
-                Protocol::PullResponse(from, data) => {
+                Protocol::PullResponse(_, mut data) => {
                     check_duplicate_instance(&data)?;
-                    pull_responses.push((from, data));
+                    pull_responses.append(&mut data);
                 }
                 Protocol::PushMessage(from, data) => {
                     check_duplicate_instance(&data)?;
@@ -2487,13 +2445,10 @@ impl ClusterInfo {
             }
         }
         if self.require_stake_for_gossip(stakes) {
-            for (_, data) in &mut pull_responses {
-                retain_staked(data, stakes);
-            }
+            retain_staked(&mut pull_responses, stakes);
             for (_, data) in &mut push_messages {
                 retain_staked(data, stakes);
             }
-            pull_responses.retain(|(_, data)| !data.is_empty());
             push_messages.retain(|(_, data)| !data.is_empty());
         }
         self.handle_batch_ping_messages(ping_messages, recycler, response_sender);
@@ -2505,7 +2460,7 @@ impl ClusterInfo {
             stakes,
             response_sender,
         );
-        self.handle_batch_pull_responses(pull_responses, thread_pool, stakes, epoch_duration);
+        self.handle_batch_pull_responses(pull_responses, stakes, epoch_duration);
         self.trim_crds_table(CRDS_UNIQUE_PUBKEY_CAPACITY, stakes);
         self.handle_batch_pong_messages(pong_messages, Instant::now());
         self.handle_batch_pull_requests(
@@ -3246,18 +3201,11 @@ mod tests {
         );
         assert_eq!(
             (0, 0, 1),
-            ClusterInfo::handle_pull_response(
-                &cluster_info,
-                &entrypoint_pubkey,
-                data.clone(),
-                &timeouts
-            )
+            cluster_info.handle_pull_response(data.clone(), &timeouts)
         );
-
-        let entrypoint_pubkey2 = solana_sdk::pubkey::new_rand();
         assert_eq!(
             (1, 0, 0),
-            ClusterInfo::handle_pull_response(&cluster_info, &entrypoint_pubkey2, data, &timeouts)
+            cluster_info.handle_pull_response(data, &timeouts)
         );
     }
 
@@ -3419,36 +3367,6 @@ mod tests {
         }
     }
 
-    #[test]
-    fn test_max_legecy_snapshot_hashes_with_push_messages() {
-        let mut rng = rand::thread_rng();
-        for _ in 0..256 {
-            let snapshot_hash = LegacySnapshotHashes::new_rand(&mut rng, None);
-            let crds_value = CrdsValue::new_signed(
-                CrdsData::LegacySnapshotHashes(snapshot_hash),
-                &Keypair::new(),
-            );
-            let message = Protocol::PushMessage(Pubkey::new_unique(), vec![crds_value]);
-            let socket = new_rand_socket_addr(&mut rng);
-            assert!(Packet::from_data(Some(&socket), message).is_ok());
-        }
-    }
-
-    #[test]
-    fn test_max_legacy_snapshot_hashes_with_pull_responses() {
-        let mut rng = rand::thread_rng();
-        for _ in 0..256 {
-            let snapshot_hash = LegacySnapshotHashes::new_rand(&mut rng, None);
-            let crds_value = CrdsValue::new_signed(
-                CrdsData::LegacySnapshotHashes(snapshot_hash),
-                &Keypair::new(),
-            );
-            let response = Protocol::PullResponse(Pubkey::new_unique(), vec![crds_value]);
-            let socket = new_rand_socket_addr(&mut rng);
-            assert!(Packet::from_data(Some(&socket), response).is_ok());
-        }
-    }
-
     #[test]
     fn test_max_snapshot_hashes_with_push_messages() {
         let mut rng = rand::thread_rng();
@@ -3720,12 +3638,11 @@ mod tests {
             &SocketAddrSpace::Unspecified,
         );
         //check that all types of gossip messages are signed correctly
-        let (push_messages, _, _) = cluster_info.gossip.new_push_messages(
-            &cluster_info.id(),
-            cluster_info.drain_push_queue(),
-            timestamp(),
-            &stakes,
-        );
+        cluster_info.flush_push_queue();
+        let (push_messages, _, _) =
+            cluster_info
+                .gossip
+                .new_push_messages(&cluster_info.id(), timestamp(), &stakes);
         // there should be some pushes ready
         assert!(!push_messages.is_empty());
         push_messages
@@ -4045,12 +3962,7 @@ mod tests {
             &stakes,
             Duration::from_millis(cluster_info.gossip.pull.crds_timeout),
         );
-        ClusterInfo::handle_pull_response(
-            &cluster_info,
-            &entrypoint_pubkey,
-            vec![entrypoint_crdsvalue],
-            &timeouts,
-        );
+        cluster_info.handle_pull_response(vec![entrypoint_crdsvalue], &timeouts);
         let (pings, pulls) = cluster_info.new_pull_requests(&thread_pool, None, &HashMap::new());
         assert_eq!(pings.len(), 1);
         assert_eq!(pulls.len(), MIN_NUM_BLOOM_FILTERS);
@@ -4084,7 +3996,7 @@ mod tests {
             ClusterInfo::split_gossip_messages(PUSH_MESSAGE_MAX_PAYLOAD_SIZE, values.clone())
                 .collect();
         let self_pubkey = solana_sdk::pubkey::new_rand();
-        assert!(splits.len() * 3 < NUM_CRDS_VALUES);
+        assert!(splits.len() * 2 < NUM_CRDS_VALUES);
         // Assert that all messages are included in the splits.
         assert_eq!(NUM_CRDS_VALUES, splits.iter().map(Vec::len).sum::<usize>());
         splits
@@ -4116,16 +4028,15 @@ mod tests {
     fn test_split_messages_packet_size() {
         // Test that if a value is smaller than payload size but too large to be wrapped in a vec
         // that it is still dropped
-        let mut value =
-            CrdsValue::new_unsigned(CrdsData::LegacySnapshotHashes(LegacySnapshotHashes {
-                from: Pubkey::default(),
-                hashes: vec![],
-                wallclock: 0,
-            }));
+        let mut value = CrdsValue::new_unsigned(CrdsData::AccountsHashes(AccountsHashes {
+            from: Pubkey::default(),
+            hashes: vec![],
+            wallclock: 0,
+        }));
 
         let mut i = 0;
         while value.size() < PUSH_MESSAGE_MAX_PAYLOAD_SIZE as u64 {
-            value.data = CrdsData::LegacySnapshotHashes(LegacySnapshotHashes {
+            value.data = CrdsData::AccountsHashes(AccountsHashes {
                 from: Pubkey::default(),
                 hashes: vec![(0, Hash::default()); i],
                 wallclock: 0,
@@ -4560,7 +4471,7 @@ mod tests {
         );
         assert_eq!(
             (0, 0, NO_ENTRIES),
-            cluster_info.handle_pull_response(&entrypoint_pubkey, data, &timeouts)
+            cluster_info.handle_pull_response(data, &timeouts)
         );
     }
 
@@ -4623,4 +4534,73 @@ mod tests {
             assert_eq!(shred_data.chunk_index() as usize, i);
         }
     }
+
+    #[test]
+    fn test_push_restart_last_voted_fork_slots() {
+        let keypair = Arc::new(Keypair::new());
+        let contact_info = ContactInfo::new_localhost(&keypair.pubkey(), 0);
+        let cluster_info = ClusterInfo::new(contact_info, keypair, SocketAddrSpace::Unspecified);
+        let slots = cluster_info.get_restart_last_voted_fork_slots(&mut Cursor::default());
+        assert!(slots.is_empty());
+        let mut update: Vec<Slot> = vec![0];
+        for i in 0..81 {
+            for j in 0..1000 {
+                update.push(i * 1050 + j);
+            }
+        }
+        assert!(cluster_info
+            .push_restart_last_voted_fork_slots(&update, Hash::default())
+            .is_ok());
+        cluster_info.flush_push_queue();
+
+        let mut cursor = Cursor::default();
+        let slots = cluster_info.get_restart_last_voted_fork_slots(&mut cursor);
+        assert_eq!(slots.len(), 1);
+        let retrieved_slots = slots[0].to_slots(0);
+        assert!(retrieved_slots[0] < 69000);
+        assert_eq!(retrieved_slots.last(), Some(84999).as_ref());
+
+        let slots = cluster_info.get_restart_last_voted_fork_slots(&mut cursor);
+        assert!(slots.is_empty());
+
+        // Test with different shred versions.
+        let mut rng = rand::thread_rng();
+        let node_pubkey = Pubkey::new_unique();
+        let mut node = LegacyContactInfo::new_rand(&mut rng, Some(node_pubkey));
+        node.set_shred_version(42);
+        let mut slots = RestartLastVotedForkSlots::new_rand(&mut rng, Some(node_pubkey));
+        slots.shred_version = 42;
+        let entries = vec![
+            CrdsValue::new_unsigned(CrdsData::LegacyContactInfo(node)),
+            CrdsValue::new_unsigned(CrdsData::RestartLastVotedForkSlots(slots)),
+        ];
+        {
+            let mut gossip_crds = cluster_info.gossip.crds.write().unwrap();
+            for entry in entries {
+                assert!(gossip_crds
+                    .insert(entry, /*now=*/ 0, GossipRoute::LocalMessage)
+                    .is_ok());
+            }
+        }
+        // Should exclude other node's last-voted-fork-slot because of different
+        // shred-version.
+        let slots = cluster_info.get_restart_last_voted_fork_slots(&mut Cursor::default());
+        assert_eq!(slots.len(), 1);
+        assert_eq!(slots[0].from, cluster_info.id());
+
+        // Match shred versions.
+        {
+            let mut node = cluster_info.my_contact_info.write().unwrap();
+            node.set_shred_version(42);
+        }
+        assert!(cluster_info
+            .push_restart_last_voted_fork_slots(&update, Hash::default())
+            .is_ok());
+        cluster_info.flush_push_queue();
+        // Should now include both slots.
+        let slots = cluster_info.get_restart_last_voted_fork_slots(&mut Cursor::default());
+        assert_eq!(slots.len(), 2);
+        assert_eq!(slots[0].from, node_pubkey);
+        assert_eq!(slots[1].from, cluster_info.id());
+    }
 }
diff --git a/gossip/src/cluster_info_metrics.rs b/gossip/src/cluster_info_metrics.rs
index 095848fd29..eed11f8313 100644
--- a/gossip/src/cluster_info_metrics.rs
+++ b/gossip/src/cluster_info_metrics.rs
@@ -315,6 +315,11 @@ pub(crate) fn submit_gossip_stats(
             stats.process_pull_response_timeout.clear(),
             i64
         ),
+        (
+            "num_redundant_pull_responses",
+            crds_stats.num_redundant_pull_responses,
+            i64
+        ),
         (
             "push_response_count",
             stats.push_response_count.clear(),
@@ -429,6 +434,11 @@ pub(crate) fn submit_gossip_stats(
             i64
         ),
         ("push_message_count", stats.push_message_count.clear(), i64),
+        (
+            "num_duplicate_push_messages",
+            crds_stats.num_duplicate_push_messages,
+            i64
+        ),
         (
             "push_fanout_num_entries",
             stats.push_fanout_num_entries.clear(),
@@ -627,6 +637,18 @@ pub(crate) fn submit_gossip_stats(
         ("SnapshotHashes-pull", crds_stats.pull.counts[10], i64),
         ("ContactInfo-push", crds_stats.push.counts[11], i64),
         ("ContactInfo-pull", crds_stats.pull.counts[11], i64),
+        (
+            "RestartLastVotedForkSlots-push",
+            crds_stats.push.counts[12],
+            i64
+        ),
+        (
+            "RestartLastVotedForkSlots-pull",
+            crds_stats.pull.counts[12],
+            i64
+        ),
+        ("RestartHeaviestFork-push", crds_stats.push.counts[13], i64),
+        ("RestartHeaviestFork-pull", crds_stats.pull.counts[13], i64),
         (
             "all-push",
             crds_stats.push.counts.iter().sum::<usize>(),
@@ -664,6 +686,18 @@ pub(crate) fn submit_gossip_stats(
         ("SnapshotHashes-pull", crds_stats.pull.fails[10], i64),
         ("ContactInfo-push", crds_stats.push.fails[11], i64),
         ("ContactInfo-pull", crds_stats.pull.fails[11], i64),
+        (
+            "RestartLastVotedForkSlots-push",
+            crds_stats.push.fails[12],
+            i64
+        ),
+        (
+            "RestartLastVotedForkSlots-pull",
+            crds_stats.pull.fails[12],
+            i64
+        ),
+        ("RestartHeaviestFork-push", crds_stats.push.fails[13], i64),
+        ("RestartHeaviestFork-pull", crds_stats.pull.fails[13], i64),
         ("all-push", crds_stats.push.fails.iter().sum::<usize>(), i64),
         ("all-pull", crds_stats.pull.fails.iter().sum::<usize>(), i64),
     );
diff --git a/gossip/src/contact_info.rs b/gossip/src/contact_info.rs
index b09957f2ce..b3ca9c94a7 100644
--- a/gossip/src/contact_info.rs
+++ b/gossip/src/contact_info.rs
@@ -350,7 +350,7 @@ impl ContactInfo {
     }
 
     // Removes the IP address at the given index if
-    // no socket entry refrences that index.
+    // no socket entry references that index.
     fn maybe_remove_addr(&mut self, index: u8) {
         if !self.sockets.iter().any(|entry| entry.index == index) {
             self.addrs.remove(usize::from(index));
diff --git a/gossip/src/crds.rs b/gossip/src/crds.rs
index d8ab6e45b3..210c7a05aa 100644
--- a/gossip/src/crds.rs
+++ b/gossip/src/crds.rs
@@ -103,7 +103,7 @@ pub enum GossipRoute<'a> {
     PushMessage(/*from:*/ &'a Pubkey),
 }
 
-type CrdsCountsArray = [usize; 12];
+type CrdsCountsArray = [usize; 14];
 
 pub(crate) struct CrdsDataStats {
     pub(crate) counts: CrdsCountsArray,
@@ -115,6 +115,10 @@ pub(crate) struct CrdsDataStats {
 pub(crate) struct CrdsStats {
     pub(crate) pull: CrdsDataStats,
     pub(crate) push: CrdsDataStats,
+    /// number of times a message was first received via a PullResponse
+    /// and that message was later received via a PushMessage
+    pub(crate) num_redundant_pull_responses: u64,
+    pub(crate) num_duplicate_push_messages: u64,
 }
 
 /// This structure stores some local metadata associated with the CrdsValue
@@ -127,8 +131,10 @@ pub struct VersionedCrdsValue {
     pub(crate) local_timestamp: u64,
     /// value hash
     pub(crate) value_hash: Hash,
-    /// Number of times duplicates of this value are recevied from gossip push.
-    num_push_dups: u8,
+    /// None -> value upserted by GossipRoute::{LocalMessage,PullRequest}
+    /// Some(0) -> value upserted by GossipRoute::PullResponse
+    /// Some(k) if k > 0 -> value upserted by GossipRoute::PushMessage w/ k - 1 push duplicates
+    num_push_recv: Option<u8>,
 }
 
 #[derive(Clone, Copy, Default)]
@@ -147,14 +153,21 @@ impl Cursor {
 }
 
 impl VersionedCrdsValue {
-    fn new(value: CrdsValue, cursor: Cursor, local_timestamp: u64) -> Self {
+    fn new(value: CrdsValue, cursor: Cursor, local_timestamp: u64, route: GossipRoute) -> Self {
         let value_hash = hash(&serialize(&value).unwrap());
+        let num_push_recv = match route {
+            GossipRoute::LocalMessage => None,
+            GossipRoute::PullRequest => None,
+            GossipRoute::PullResponse => Some(0),
+            GossipRoute::PushMessage(_) => Some(1),
+        };
+
         VersionedCrdsValue {
             ordinal: cursor.ordinal(),
             value,
             local_timestamp,
             value_hash,
-            num_push_dups: 0u8,
+            num_push_recv,
         }
     }
 }
@@ -222,10 +235,11 @@ impl Crds {
     ) -> Result<(), CrdsError> {
         let label = value.label();
         let pubkey = value.pubkey();
-        let value = VersionedCrdsValue::new(value, self.cursor, now);
+        let value = VersionedCrdsValue::new(value, self.cursor, now, route);
+        let mut stats = self.stats.lock().unwrap();
         match self.table.entry(label) {
             Entry::Vacant(entry) => {
-                self.stats.lock().unwrap().record_insert(&value, route);
+                stats.record_insert(&value, route);
                 let entry_index = entry.index();
                 self.shards.insert(entry_index, &value);
                 match &value.value.data {
@@ -251,7 +265,7 @@ impl Crds {
                 Ok(())
             }
             Entry::Occupied(mut entry) if overrides(&value.value, entry.get()) => {
-                self.stats.lock().unwrap().record_insert(&value, route);
+                stats.record_insert(&value, route);
                 let entry_index = entry.index();
                 self.shards.remove(entry_index, entry.get());
                 self.shards.insert(entry_index, &value);
@@ -290,7 +304,7 @@ impl Crds {
                 Ok(())
             }
             Entry::Occupied(mut entry) => {
-                self.stats.lock().unwrap().record_fail(&value, route);
+                stats.record_fail(&value, route);
                 trace!(
                     "INSERT FAILED data: {} new.wallclock: {}",
                     value.value.label(),
@@ -303,8 +317,14 @@ impl Crds {
                     Err(CrdsError::InsertFailed)
                 } else if matches!(route, GossipRoute::PushMessage(_)) {
                     let entry = entry.get_mut();
-                    entry.num_push_dups = entry.num_push_dups.saturating_add(1);
-                    Err(CrdsError::DuplicatePush(entry.num_push_dups))
+                    if entry.num_push_recv == Some(0) {
+                        stats.num_redundant_pull_responses += 1;
+                    } else {
+                        stats.num_duplicate_push_messages += 1;
+                    }
+                    let num_push_dups = entry.num_push_recv.unwrap_or_default();
+                    entry.num_push_recv = Some(num_push_dups.saturating_add(1));
+                    Err(CrdsError::DuplicatePush(num_push_dups))
                 } else {
                     Err(CrdsError::InsertFailed)
                 }
@@ -721,6 +741,8 @@ impl CrdsDataStats {
             CrdsData::DuplicateShred(_, _) => 9,
             CrdsData::SnapshotHashes(_) => 10,
             CrdsData::ContactInfo(_) => 11,
+            CrdsData::RestartLastVotedForkSlots(_) => 12,
+            CrdsData::RestartHeaviestFork(_) => 13,
             // Update CrdsCountsArray if new items are added here.
         }
     }
@@ -759,7 +781,7 @@ fn should_report_message_signature(signature: &Signature) -> bool {
 mod tests {
     use {
         super::*,
-        crate::crds_value::{new_rand_timestamp, LegacySnapshotHashes, NodeInstance},
+        crate::crds_value::{new_rand_timestamp, AccountsHashes, NodeInstance},
         rand::{thread_rng, Rng, SeedableRng},
         rand_chacha::ChaChaRng,
         rayon::ThreadPoolBuilder,
@@ -1341,8 +1363,8 @@ mod tests {
         );
         assert_eq!(crds.get_shred_version(&pubkey), Some(8));
         // Add other crds values with the same pubkey.
-        let val = LegacySnapshotHashes::new_rand(&mut rng, Some(pubkey));
-        let val = CrdsData::LegacySnapshotHashes(val);
+        let val = AccountsHashes::new_rand(&mut rng, Some(pubkey));
+        let val = CrdsData::AccountsHashes(val);
         let val = CrdsValue::new_unsigned(val);
         assert_eq!(
             crds.insert(val, timestamp(), GossipRoute::LocalMessage),
@@ -1355,7 +1377,7 @@ mod tests {
         assert_eq!(crds.get::<&ContactInfo>(pubkey), None);
         assert_eq!(crds.get_shred_version(&pubkey), Some(8));
         // Remove the remaining entry with the same pubkey.
-        crds.remove(&CrdsValueLabel::LegacySnapshotHashes(pubkey), timestamp());
+        crds.remove(&CrdsValueLabel::AccountsHashes(pubkey), timestamp());
         assert_eq!(crds.get_records(&pubkey).count(), 0);
         assert_eq!(crds.get_shred_version(&pubkey), None);
     }
@@ -1448,8 +1470,9 @@ mod tests {
     #[allow(clippy::neg_cmp_op_on_partial_ord)]
     fn test_equal() {
         let val = CrdsValue::new_unsigned(CrdsData::LegacyContactInfo(ContactInfo::default()));
-        let v1 = VersionedCrdsValue::new(val.clone(), Cursor::default(), 1);
-        let v2 = VersionedCrdsValue::new(val, Cursor::default(), 1);
+        let v1 =
+            VersionedCrdsValue::new(val.clone(), Cursor::default(), 1, GossipRoute::LocalMessage);
+        let v2 = VersionedCrdsValue::new(val, Cursor::default(), 1, GossipRoute::LocalMessage);
         assert_eq!(v1, v2);
         assert!(!(v1 != v2));
         assert!(!overrides(&v1.value, &v2));
@@ -1465,6 +1488,7 @@ mod tests {
             ))),
             Cursor::default(),
             1, // local_timestamp
+            GossipRoute::LocalMessage,
         );
         let v2 = VersionedCrdsValue::new(
             {
@@ -1474,6 +1498,7 @@ mod tests {
             },
             Cursor::default(),
             1, // local_timestamp
+            GossipRoute::LocalMessage,
         );
 
         assert_eq!(v1.value.label(), v2.value.label());
@@ -1499,6 +1524,7 @@ mod tests {
             ))),
             Cursor::default(),
             1, // local_timestamp
+            GossipRoute::LocalMessage,
         );
         let v2 = VersionedCrdsValue::new(
             CrdsValue::new_unsigned(CrdsData::LegacyContactInfo(ContactInfo::new_localhost(
@@ -1507,6 +1533,7 @@ mod tests {
             ))),
             Cursor::default(),
             1, // local_timestamp
+            GossipRoute::LocalMessage,
         );
         assert_eq!(v1.value.label(), v2.value.label());
         assert!(overrides(&v1.value, &v2));
@@ -1525,6 +1552,7 @@ mod tests {
             ))),
             Cursor::default(),
             1, // local_timestamp
+            GossipRoute::LocalMessage,
         );
         let v2 = VersionedCrdsValue::new(
             CrdsValue::new_unsigned(CrdsData::LegacyContactInfo(ContactInfo::new_localhost(
@@ -1533,6 +1561,7 @@ mod tests {
             ))),
             Cursor::default(),
             1, // local_timestamp
+            GossipRoute::LocalMessage,
         );
         assert_ne!(v1, v2);
         assert!(!(v1 == v2));
diff --git a/gossip/src/crds_entry.rs b/gossip/src/crds_entry.rs
index ccb8ed310e..526f04eb56 100644
--- a/gossip/src/crds_entry.rs
+++ b/gossip/src/crds_entry.rs
@@ -2,8 +2,7 @@ use {
     crate::{
         crds::VersionedCrdsValue,
         crds_value::{
-            CrdsData, CrdsValue, CrdsValueLabel, LegacySnapshotHashes, LegacyVersion, LowestSlot,
-            SnapshotHashes, Version,
+            CrdsData, CrdsValue, CrdsValueLabel, LegacyVersion, LowestSlot, SnapshotHashes, Version,
         },
         legacy_contact_info::LegacyContactInfo,
     },
@@ -57,11 +56,6 @@ impl_crds_entry!(LegacyContactInfo, CrdsData::LegacyContactInfo(node), node);
 impl_crds_entry!(LegacyVersion, CrdsData::LegacyVersion(version), version);
 impl_crds_entry!(LowestSlot, CrdsData::LowestSlot(_, slot), slot);
 impl_crds_entry!(Version, CrdsData::Version(version), version);
-impl_crds_entry!(
-    LegacySnapshotHashes,
-    CrdsData::LegacySnapshotHashes(snapshot_hashes),
-    snapshot_hashes
-);
 impl_crds_entry!(
     SnapshotHashes,
     CrdsData::SnapshotHashes(snapshot_hashes),
@@ -118,9 +112,6 @@ mod tests {
                 CrdsData::LegacyVersion(version) => {
                     assert_eq!(crds.get::<&LegacyVersion>(key), Some(version))
                 }
-                CrdsData::LegacySnapshotHashes(hash) => {
-                    assert_eq!(crds.get::<&LegacySnapshotHashes>(key), Some(hash))
-                }
                 CrdsData::SnapshotHashes(hash) => {
                     assert_eq!(crds.get::<&SnapshotHashes>(key), Some(hash))
                 }
diff --git a/gossip/src/crds_gossip.rs b/gossip/src/crds_gossip.rs
index 015deed1d2..977db716e5 100644
--- a/gossip/src/crds_gossip.rs
+++ b/gossip/src/crds_gossip.rs
@@ -72,7 +72,6 @@ impl CrdsGossip {
     pub fn new_push_messages(
         &self,
         pubkey: &Pubkey, // This node.
-        pending_push_messages: Vec<CrdsValue>,
         now: u64,
         stakes: &HashMap<Pubkey, u64>,
     ) -> (
@@ -80,12 +79,6 @@ impl CrdsGossip {
         usize, // number of values
         usize, // number of push messages
     ) {
-        {
-            let mut crds = self.crds.write().unwrap();
-            for entry in pending_push_messages {
-                let _ = crds.insert(entry, now, GossipRoute::LocalMessage);
-            }
-        }
         self.push.new_push_messages(pubkey, &self.crds, now, stakes)
     }
 
diff --git a/gossip/src/crds_gossip_pull.rs b/gossip/src/crds_gossip_pull.rs
index 191406dd67..7f70e79bf0 100644
--- a/gossip/src/crds_gossip_pull.rs
+++ b/gossip/src/crds_gossip_pull.rs
@@ -28,7 +28,7 @@ use {
         Rng,
     },
     rayon::{prelude::*, ThreadPool},
-    solana_bloom::bloom::{AtomicBloom, Bloom},
+    solana_bloom::bloom::{Bloom, ConcurrentBloom},
     solana_sdk::{
         hash::{hash, Hash},
         native_token::LAMPORTS_PER_SOL,
@@ -141,7 +141,7 @@ impl CrdsFilter {
 
 /// A vector of crds filters that together hold a complete set of Hashes.
 struct CrdsFilterSet {
-    filters: Vec<Option<AtomicBloom<Hash>>>,
+    filters: Vec<Option<ConcurrentBloom<Hash>>>,
     mask_bits: u32,
 }
 
@@ -159,7 +159,7 @@ impl CrdsFilterSet {
             let k = rng.gen_range(0..indices.len());
             let k = indices.swap_remove(k);
             let filter = Bloom::random(max_items as usize, FALSE_RATE, max_bits as usize);
-            filters[k] = Some(AtomicBloom::<Hash>::from(filter));
+            filters[k] = Some(ConcurrentBloom::<Hash>::from(filter));
         }
         Self { filters, mask_bits }
     }
diff --git a/gossip/src/crds_value.rs b/gossip/src/crds_value.rs
index 87ba34604e..ad6422fc2e 100644
--- a/gossip/src/crds_value.rs
+++ b/gossip/src/crds_value.rs
@@ -1,11 +1,12 @@
 use {
     crate::{
-        cluster_info::MAX_LEGACY_SNAPSHOT_HASHES,
+        cluster_info::MAX_ACCOUNTS_HASHES,
         contact_info::ContactInfo,
         deprecated,
         duplicate_shred::{DuplicateShred, DuplicateShredIndex, MAX_DUPLICATE_SHREDS},
         epoch_slots::EpochSlots,
         legacy_contact_info::LegacyContactInfo,
+        restart_crds_values::{RestartHeaviestFork, RestartLastVotedForkSlots},
     },
     bincode::{serialize, serialized_size},
     rand::{CryptoRng, Rng},
@@ -85,7 +86,7 @@ pub enum CrdsData {
     LegacyContactInfo(LegacyContactInfo),
     Vote(VoteIndex, Vote),
     LowestSlot(/*DEPRECATED:*/ u8, LowestSlot),
-    LegacySnapshotHashes(LegacySnapshotHashes),
+    LegacySnapshotHashes(LegacySnapshotHashes), // Deprecated
     AccountsHashes(AccountsHashes),
     EpochSlots(EpochSlotsIndex, EpochSlots),
     LegacyVersion(LegacyVersion),
@@ -94,6 +95,8 @@ pub enum CrdsData {
     DuplicateShred(DuplicateShredIndex, DuplicateShred),
     SnapshotHashes(SnapshotHashes),
     ContactInfo(ContactInfo),
+    RestartLastVotedForkSlots(RestartLastVotedForkSlots),
+    RestartHeaviestFork(RestartHeaviestFork),
 }
 
 impl Sanitize for CrdsData {
@@ -132,6 +135,8 @@ impl Sanitize for CrdsData {
             }
             CrdsData::SnapshotHashes(val) => val.sanitize(),
             CrdsData::ContactInfo(node) => node.sanitize(),
+            CrdsData::RestartLastVotedForkSlots(slots) => slots.sanitize(),
+            CrdsData::RestartHeaviestFork(fork) => fork.sanitize(),
         }
     }
 }
@@ -145,7 +150,7 @@ pub(crate) fn new_rand_timestamp<R: Rng>(rng: &mut R) -> u64 {
 impl CrdsData {
     /// New random CrdsData for tests and benchmarks.
     fn new_rand<R: Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> CrdsData {
-        let kind = rng.gen_range(0..7);
+        let kind = rng.gen_range(0..9);
         // TODO: Implement other kinds of CrdsData here.
         // TODO: Assign ranges to each arm proportional to their frequency in
         // the mainnet crds table.
@@ -157,6 +162,10 @@ impl CrdsData {
             3 => CrdsData::AccountsHashes(AccountsHashes::new_rand(rng, pubkey)),
             4 => CrdsData::Version(Version::new_rand(rng, pubkey)),
             5 => CrdsData::Vote(rng.gen_range(0..MAX_VOTES), Vote::new_rand(rng, pubkey)),
+            6 => CrdsData::RestartLastVotedForkSlots(RestartLastVotedForkSlots::new_rand(
+                rng, pubkey,
+            )),
+            7 => CrdsData::RestartHeaviestFork(RestartHeaviestFork::new_rand(rng, pubkey)),
             _ => CrdsData::EpochSlots(
                 rng.gen_range(0..MAX_EPOCH_SLOTS),
                 EpochSlots::new_rand(rng, pubkey),
@@ -195,7 +204,7 @@ impl AccountsHashes {
 
     /// New random AccountsHashes for tests and benchmarks.
     pub(crate) fn new_rand<R: Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> Self {
-        let num_hashes = rng.gen_range(0..MAX_LEGACY_SNAPSHOT_HASHES) + 1;
+        let num_hashes = rng.gen_range(0..MAX_ACCOUNTS_HASHES) + 1;
         let hashes = std::iter::repeat_with(|| {
             let slot = 47825632 + rng.gen_range(0..512);
             let hash = Hash::new_unique();
@@ -211,7 +220,7 @@ impl AccountsHashes {
     }
 }
 
-pub type LegacySnapshotHashes = AccountsHashes;
+type LegacySnapshotHashes = AccountsHashes;
 
 #[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, AbiExample)]
 pub struct SnapshotHashes {
@@ -501,6 +510,8 @@ pub enum CrdsValueLabel {
     DuplicateShred(DuplicateShredIndex, Pubkey),
     SnapshotHashes(Pubkey),
     ContactInfo(Pubkey),
+    RestartLastVotedForkSlots(Pubkey),
+    RestartHeaviestFork(Pubkey),
 }
 
 impl fmt::Display for CrdsValueLabel {
@@ -524,6 +535,12 @@ impl fmt::Display for CrdsValueLabel {
                 write!(f, "SnapshotHashes({})", self.pubkey())
             }
             CrdsValueLabel::ContactInfo(_) => write!(f, "ContactInfo({})", self.pubkey()),
+            CrdsValueLabel::RestartLastVotedForkSlots(_) => {
+                write!(f, "RestartLastVotedForkSlots({})", self.pubkey())
+            }
+            CrdsValueLabel::RestartHeaviestFork(_) => {
+                write!(f, "RestartHeaviestFork({})", self.pubkey())
+            }
         }
     }
 }
@@ -543,6 +560,8 @@ impl CrdsValueLabel {
             CrdsValueLabel::DuplicateShred(_, p) => *p,
             CrdsValueLabel::SnapshotHashes(p) => *p,
             CrdsValueLabel::ContactInfo(pubkey) => *pubkey,
+            CrdsValueLabel::RestartLastVotedForkSlots(p) => *p,
+            CrdsValueLabel::RestartHeaviestFork(p) => *p,
         }
     }
 }
@@ -593,6 +612,8 @@ impl CrdsValue {
             CrdsData::DuplicateShred(_, shred) => shred.wallclock,
             CrdsData::SnapshotHashes(hash) => hash.wallclock,
             CrdsData::ContactInfo(node) => node.wallclock(),
+            CrdsData::RestartLastVotedForkSlots(slots) => slots.wallclock,
+            CrdsData::RestartHeaviestFork(fork) => fork.wallclock,
         }
     }
     pub fn pubkey(&self) -> Pubkey {
@@ -609,6 +630,8 @@ impl CrdsValue {
             CrdsData::DuplicateShred(_, shred) => shred.from,
             CrdsData::SnapshotHashes(hash) => hash.from,
             CrdsData::ContactInfo(node) => *node.pubkey(),
+            CrdsData::RestartLastVotedForkSlots(slots) => slots.from,
+            CrdsData::RestartHeaviestFork(fork) => fork.from,
         }
     }
     pub fn label(&self) -> CrdsValueLabel {
@@ -627,6 +650,10 @@ impl CrdsValue {
             CrdsData::DuplicateShred(ix, shred) => CrdsValueLabel::DuplicateShred(*ix, shred.from),
             CrdsData::SnapshotHashes(_) => CrdsValueLabel::SnapshotHashes(self.pubkey()),
             CrdsData::ContactInfo(node) => CrdsValueLabel::ContactInfo(*node.pubkey()),
+            CrdsData::RestartLastVotedForkSlots(_) => {
+                CrdsValueLabel::RestartLastVotedForkSlots(self.pubkey())
+            }
+            CrdsData::RestartHeaviestFork(_) => CrdsValueLabel::RestartHeaviestFork(self.pubkey()),
         }
     }
     pub fn contact_info(&self) -> Option<&LegacyContactInfo> {
@@ -1050,7 +1077,7 @@ mod test {
         assert!(!other.check_duplicate(&node_crds));
         assert_eq!(node.overrides(&other_crds), None);
         assert_eq!(other.overrides(&node_crds), None);
-        // Differnt crds value is not a duplicate.
+        // Different crds value is not a duplicate.
         let other = LegacyContactInfo::new_rand(&mut rng, Some(pubkey));
         let other = CrdsValue::new_unsigned(CrdsData::LegacyContactInfo(other));
         assert!(!node.check_duplicate(&other));
diff --git a/gossip/src/duplicate_shred.rs b/gossip/src/duplicate_shred.rs
index b1ceab79b2..84c50ea602 100644
--- a/gossip/src/duplicate_shred.rs
+++ b/gossip/src/duplicate_shred.rs
@@ -30,7 +30,7 @@ pub struct DuplicateShred {
     pub(crate) wallclock: u64,
     pub(crate) slot: Slot,
     _unused: u32,
-    shred_type: ShredType,
+    _unused_shred_type: ShredType,
     // Serialized DuplicateSlotProof split into chunks.
     num_chunks: u8,
     chunk_index: u8,
@@ -56,6 +56,8 @@ pub enum Error {
     BlockstoreInsertFailed(#[from] BlockstoreError),
     #[error("data chunk mismatch")]
     DataChunkMismatch,
+    #[error("unable to send duplicate slot to state machine")]
+    DuplicateSlotSenderFailure,
     #[error("invalid chunk_index: {chunk_index}, num_chunks: {num_chunks}")]
     InvalidChunkIndex { chunk_index: u8, num_chunks: u8 },
     #[error("invalid duplicate shreds")]
@@ -90,8 +92,8 @@ pub enum Error {
 
 /// Check that `shred1` and `shred2` indicate a valid duplicate proof
 ///     - Must be for the same slot
-///     - Must have the same `shred_type`
 ///     - Must both sigverify for the correct leader
+///     - Must have a merkle root conflict, otherwise `shred1` and `shred2` must have the same `shred_type`
 ///     - If `shred1` and `shred2` share the same index they must be not equal
 ///     - If `shred1` and `shred2` do not share the same index and are data shreds
 ///       verify that they indicate an index conflict. One of them must be the
@@ -106,10 +108,6 @@ where
         return Err(Error::SlotMismatch);
     }
 
-    if shred1.shred_type() != shred2.shred_type() {
-        return Err(Error::ShredTypeMismatch);
-    }
-
     if let Some(leader_schedule) = leader_schedule {
         let slot_leader =
             leader_schedule(shred1.slot()).ok_or(Error::UnknownSlotLeader(shred1.slot()))?;
@@ -118,6 +116,20 @@ where
         }
     }
 
+    // Merkle root conflict check
+    if shred1.fec_set_index() == shred2.fec_set_index()
+        && shred1.merkle_root().ok() != shred2.merkle_root().ok()
+    {
+        // This catches a mixture of legacy and merkle shreds
+        // as well as merkle shreds with different roots in the
+        // same fec set
+        return Ok(());
+    }
+
+    if shred1.shred_type() != shred2.shred_type() {
+        return Err(Error::ShredTypeMismatch);
+    }
+
     if shred1.index() == shred2.index() {
         if shred1.payload() != shred2.payload() {
             return Ok(());
@@ -164,7 +176,7 @@ where
     }
     let other_shred = Shred::new_from_serialized_shred(other_payload)?;
     check_shreds(leader_schedule, &shred, &other_shred)?;
-    let (slot, shred_type) = (shred.slot(), shred.shred_type());
+    let slot = shred.slot();
     let proof = DuplicateSlotProof {
         shred1: shred.into_payload(),
         shred2: other_shred.into_payload(),
@@ -184,27 +196,21 @@ where
             from: self_pubkey,
             wallclock,
             slot,
-            shred_type,
             num_chunks,
             chunk_index: i as u8,
             chunk,
             _unused: 0,
+            _unused_shred_type: ShredType::Code,
         });
     Ok(chunks)
 }
 
 // Returns a predicate checking if a duplicate-shred chunk matches
-// (slot, shred_type) and has valid chunk_index.
-fn check_chunk(
-    slot: Slot,
-    shred_type: ShredType,
-    num_chunks: u8,
-) -> impl Fn(&DuplicateShred) -> Result<(), Error> {
+// the slot and has valid chunk_index.
+fn check_chunk(slot: Slot, num_chunks: u8) -> impl Fn(&DuplicateShred) -> Result<(), Error> {
     move |dup| {
         if dup.slot != slot {
             Err(Error::SlotMismatch)
-        } else if dup.shred_type != shred_type {
-            Err(Error::ShredTypeMismatch)
         } else if dup.num_chunks != num_chunks {
             Err(Error::NumChunksMismatch)
         } else if dup.chunk_index >= num_chunks {
@@ -226,13 +232,12 @@ pub(crate) fn into_shreds(
     let mut chunks = chunks.into_iter();
     let DuplicateShred {
         slot,
-        shred_type,
         num_chunks,
         chunk_index,
         chunk,
         ..
     } = chunks.next().ok_or(Error::InvalidDuplicateShreds)?;
-    let check_chunk = check_chunk(slot, shred_type, num_chunks);
+    let check_chunk = check_chunk(slot, num_chunks);
     let mut data = HashMap::new();
     data.insert(chunk_index, chunk);
     for chunk in chunks {
@@ -260,8 +265,6 @@ pub(crate) fn into_shreds(
     let shred2 = Shred::new_from_serialized_shred(proof.shred2)?;
     if shred1.slot() != slot || shred2.slot() != slot {
         Err(Error::SlotMismatch)
-    } else if shred1.shred_type() != shred_type || shred2.shred_type() != shred_type {
-        Err(Error::ShredTypeMismatch)
     } else {
         check_shreds(Some(|_| Some(slot_leader).copied()), &shred1, &shred2)?;
         Ok((shred1, shred2))
@@ -300,7 +303,7 @@ pub(crate) mod tests {
             from: Pubkey::new_unique(),
             wallclock: u64::MAX,
             slot: Slot::MAX,
-            shred_type: ShredType::Data,
+            _unused_shred_type: ShredType::Data,
             num_chunks: u8::MAX,
             chunk_index: u8::MAX,
             chunk: Vec::default(),
@@ -406,6 +409,8 @@ pub(crate) mod tests {
             keypair,
             &entries,
             is_last_in_slot,
+            // chained_merkle_root
+            Some(Hash::new_from_array(rng.gen())),
             next_shred_index,
             next_code_index, // next_code_index
             merkle_variant,
@@ -421,7 +426,7 @@ pub(crate) mod tests {
         wallclock: u64,
         max_size: usize, // Maximum serialized size of each DuplicateShred.
     ) -> Result<impl Iterator<Item = DuplicateShred>, Error> {
-        let (slot, shred_type) = (shred.slot(), shred.shred_type());
+        let slot = shred.slot();
         let proof = DuplicateSlotProof {
             shred1: shred.into_payload(),
             shred2: other_shred.into_payload(),
@@ -437,11 +442,11 @@ pub(crate) mod tests {
                 from: self_pubkey,
                 wallclock,
                 slot,
-                shred_type,
                 num_chunks,
                 chunk_index: i as u8,
                 chunk,
                 _unused: 0,
+                _unused_shred_type: ShredType::Code,
             });
         Ok(chunks)
     }
@@ -949,4 +954,186 @@ pub(crate) mod tests {
             );
         }
     }
+
+    #[test]
+    fn test_merkle_root_conflict_round_trip() {
+        let mut rng = rand::thread_rng();
+        let leader = Arc::new(Keypair::new());
+        let (slot, parent_slot, reference_tick, version) = (53084024, 53084023, 0, 0);
+        let shredder = Shredder::new(slot, parent_slot, reference_tick, version).unwrap();
+        let next_shred_index = rng.gen_range(0..31_000);
+        let leader_schedule = |s| {
+            if s == slot {
+                Some(leader.pubkey())
+            } else {
+                None
+            }
+        };
+
+        let (data_shreds, coding_shreds) = new_rand_shreds(
+            &mut rng,
+            next_shred_index,
+            next_shred_index,
+            10,
+            true, /* merkle_variant */
+            &shredder,
+            &leader,
+            false,
+        );
+
+        let (legacy_data_shreds, legacy_coding_shreds) = new_rand_shreds(
+            &mut rng,
+            next_shred_index,
+            next_shred_index,
+            10,
+            false, /* merkle_variant */
+            &shredder,
+            &leader,
+            true,
+        );
+
+        let (diff_data_shreds, diff_coding_shreds) = new_rand_shreds(
+            &mut rng,
+            next_shred_index,
+            next_shred_index,
+            10,
+            true, /* merkle_variant */
+            &shredder,
+            &leader,
+            false,
+        );
+
+        let test_cases = vec![
+            (data_shreds[0].clone(), diff_data_shreds[1].clone()),
+            (coding_shreds[0].clone(), diff_coding_shreds[1].clone()),
+            (data_shreds[0].clone(), diff_coding_shreds[0].clone()),
+            (coding_shreds[0].clone(), diff_data_shreds[0].clone()),
+            // Mix of legacy and merkle for same fec set
+            (legacy_coding_shreds[0].clone(), data_shreds[0].clone()),
+            (coding_shreds[0].clone(), legacy_data_shreds[0].clone()),
+            (legacy_data_shreds[0].clone(), coding_shreds[0].clone()),
+            (data_shreds[0].clone(), legacy_coding_shreds[0].clone()),
+        ];
+        for (shred1, shred2) in test_cases.into_iter() {
+            let chunks: Vec<_> = from_shred(
+                shred1.clone(),
+                Pubkey::new_unique(), // self_pubkey
+                shred2.payload().clone(),
+                Some(leader_schedule),
+                rng.gen(), // wallclock
+                512,       // max_size
+            )
+            .unwrap()
+            .collect();
+            assert!(chunks.len() > 4);
+            let (shred3, shred4) = into_shreds(&leader.pubkey(), chunks).unwrap();
+            assert_eq!(shred1, shred3);
+            assert_eq!(shred2, shred4);
+        }
+    }
+
+    #[test]
+    fn test_merkle_root_conflict_invalid() {
+        let mut rng = rand::thread_rng();
+        let leader = Arc::new(Keypair::new());
+        let (slot, parent_slot, reference_tick, version) = (53084024, 53084023, 0, 0);
+        let shredder = Shredder::new(slot, parent_slot, reference_tick, version).unwrap();
+        let next_shred_index = rng.gen_range(0..31_000);
+        let leader_schedule = |s| {
+            if s == slot {
+                Some(leader.pubkey())
+            } else {
+                None
+            }
+        };
+
+        let (data_shreds, coding_shreds) = new_rand_shreds(
+            &mut rng,
+            next_shred_index,
+            next_shred_index,
+            10,
+            true,
+            &shredder,
+            &leader,
+            true,
+        );
+
+        let (next_data_shreds, next_coding_shreds) = new_rand_shreds(
+            &mut rng,
+            next_shred_index + 1,
+            next_shred_index + 1,
+            10,
+            true,
+            &shredder,
+            &leader,
+            true,
+        );
+
+        let (legacy_data_shreds, legacy_coding_shreds) = new_rand_shreds(
+            &mut rng,
+            next_shred_index,
+            next_shred_index,
+            10,
+            false,
+            &shredder,
+            &leader,
+            true,
+        );
+
+        let test_cases = vec![
+            // Same fec set same merkle root
+            (coding_shreds[0].clone(), data_shreds[0].clone()),
+            (data_shreds[0].clone(), coding_shreds[0].clone()),
+            // Different FEC set different merkle root
+            (coding_shreds[0].clone(), next_data_shreds[0].clone()),
+            (next_coding_shreds[0].clone(), data_shreds[0].clone()),
+            (data_shreds[0].clone(), next_coding_shreds[0].clone()),
+            (next_data_shreds[0].clone(), coding_shreds[0].clone()),
+            // Legacy shreds
+            (
+                legacy_coding_shreds[0].clone(),
+                legacy_data_shreds[0].clone(),
+            ),
+            (
+                legacy_data_shreds[0].clone(),
+                legacy_coding_shreds[0].clone(),
+            ),
+            // Mix of legacy and merkle with different fec index
+            (legacy_coding_shreds[0].clone(), next_data_shreds[0].clone()),
+            (next_coding_shreds[0].clone(), legacy_data_shreds[0].clone()),
+            (legacy_data_shreds[0].clone(), next_coding_shreds[0].clone()),
+            (next_data_shreds[0].clone(), legacy_coding_shreds[0].clone()),
+        ];
+        for (shred1, shred2) in test_cases.into_iter() {
+            assert_matches!(
+                from_shred(
+                    shred1.clone(),
+                    Pubkey::new_unique(), // self_pubkey
+                    shred2.payload().clone(),
+                    Some(leader_schedule),
+                    rng.gen(), // wallclock
+                    512,       // max_size
+                )
+                .err()
+                .unwrap(),
+                Error::ShredTypeMismatch
+            );
+
+            let chunks: Vec<_> = from_shred_bypass_checks(
+                shred1.clone(),
+                Pubkey::new_unique(), // self_pubkey
+                shred2.clone(),
+                rng.gen(), // wallclock
+                512,       // max_size
+            )
+            .unwrap()
+            .collect();
+            assert!(chunks.len() > 4);
+
+            assert_matches!(
+                into_shreds(&leader.pubkey(), chunks).err().unwrap(),
+                Error::ShredTypeMismatch
+            );
+        }
+    }
 }
diff --git a/gossip/src/duplicate_shred_handler.rs b/gossip/src/duplicate_shred_handler.rs
index ba95178bc8..e7b4cd0466 100644
--- a/gossip/src/duplicate_shred_handler.rs
+++ b/gossip/src/duplicate_shred_handler.rs
@@ -3,11 +3,13 @@ use {
         duplicate_shred::{self, DuplicateShred, Error},
         duplicate_shred_listener::DuplicateShredHandlerTrait,
     },
+    crossbeam_channel::Sender,
     log::error,
     solana_ledger::{blockstore::Blockstore, leader_schedule_cache::LeaderScheduleCache},
     solana_runtime::bank_forks::BankForks,
     solana_sdk::{
         clock::{Epoch, Slot},
+        feature_set,
         pubkey::Pubkey,
     },
     std::{
@@ -44,6 +46,8 @@ pub struct DuplicateShredHandler {
     cached_on_epoch: Epoch,
     cached_staked_nodes: Arc<HashMap<Pubkey, u64>>,
     cached_slots_in_epoch: u64,
+    // Used to notify duplicate consensus state machine
+    duplicate_slots_sender: Sender<Slot>,
 }
 
 impl DuplicateShredHandlerTrait for DuplicateShredHandler {
@@ -63,6 +67,7 @@ impl DuplicateShredHandler {
         blockstore: Arc<Blockstore>,
         leader_schedule_cache: Arc<LeaderScheduleCache>,
         bank_forks: Arc<RwLock<BankForks>>,
+        duplicate_slots_sender: Sender<Slot>,
     ) -> Self {
         Self {
             buffer: HashMap::<(Slot, Pubkey), BufferEntry>::default(),
@@ -74,11 +79,12 @@ impl DuplicateShredHandler {
             blockstore,
             leader_schedule_cache,
             bank_forks,
+            duplicate_slots_sender,
         }
     }
 
     fn cache_root_info(&mut self) {
-        let last_root = self.blockstore.last_root();
+        let last_root = self.blockstore.max_root();
         if last_root == self.last_root && !self.cached_staked_nodes.is_empty() {
             return;
         }
@@ -131,12 +137,30 @@ impl DuplicateShredHandler {
                     shred1.into_payload(),
                     shred2.into_payload(),
                 )?;
+                if self.should_notify_state_machine(slot) {
+                    // Notify duplicate consensus state machine
+                    self.duplicate_slots_sender
+                        .send(slot)
+                        .map_err(|_| Error::DuplicateSlotSenderFailure)?;
+                }
             }
             self.consumed.insert(slot, true);
         }
         Ok(())
     }
 
+    fn should_notify_state_machine(&self, slot: Slot) -> bool {
+        let root_bank = self.bank_forks.read().unwrap().root_bank();
+        let Some(activated_slot) = root_bank
+            .feature_set
+            .activated_slot(&feature_set::enable_gossip_duplicate_proof_ingestion::id())
+        else {
+            return false;
+        };
+        root_bank.epoch_schedule().get_epoch(slot)
+            > root_bank.epoch_schedule().get_epoch(activated_slot)
+    }
+
     fn should_consume_slot(&mut self, slot: Slot) -> bool {
         slot > self.last_root
             && slot < self.last_root.saturating_add(self.cached_slots_in_epoch)
@@ -211,12 +235,14 @@ mod tests {
             cluster_info::DUPLICATE_SHRED_MAX_PAYLOAD_SIZE,
             duplicate_shred::{from_shred, tests::new_rand_shred},
         },
+        crossbeam_channel::unbounded,
+        itertools::Itertools,
         solana_ledger::{
             genesis_utils::{create_genesis_config_with_leader, GenesisConfigInfo},
             get_tmp_ledger_path_auto_delete,
             shred::Shredder,
         },
-        solana_runtime::bank::Bank,
+        solana_runtime::{accounts_background_service::AbsRequestSender, bank::Bank},
         solana_sdk::{
             signature::{Keypair, Signer},
             timing::timestamp,
@@ -271,16 +297,34 @@ mod tests {
         let my_pubkey = my_keypair.pubkey();
         let genesis_config_info = create_genesis_config_with_leader(10_000, &my_pubkey, 10_000);
         let GenesisConfigInfo { genesis_config, .. } = genesis_config_info;
-        let bank_forks = BankForks::new_rw_arc(Bank::new_for_tests(&genesis_config));
+        let mut bank = Bank::new_for_tests(&genesis_config);
+        bank.activate_feature(&feature_set::enable_gossip_duplicate_proof_ingestion::id());
+        let slots_in_epoch = bank.get_epoch_info().slots_in_epoch;
+        let bank_forks_arc = BankForks::new_rw_arc(bank);
+        {
+            let mut bank_forks = bank_forks_arc.write().unwrap();
+            let bank0 = bank_forks.get(0).unwrap();
+            bank_forks.insert(Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 9));
+            bank_forks.set_root(9, &AbsRequestSender::default(), None);
+        }
+        blockstore.set_roots([0, 9].iter()).unwrap();
         let leader_schedule_cache = Arc::new(LeaderScheduleCache::new_from_bank(
-            &bank_forks.read().unwrap().working_bank(),
+            &bank_forks_arc.read().unwrap().working_bank(),
         ));
-        let mut duplicate_shred_handler =
-            DuplicateShredHandler::new(blockstore.clone(), leader_schedule_cache, bank_forks);
+        let (sender, receiver) = unbounded();
+        // The feature will only be activated at Epoch 1.
+        let start_slot: Slot = slots_in_epoch + 1;
+
+        let mut duplicate_shred_handler = DuplicateShredHandler::new(
+            blockstore.clone(),
+            leader_schedule_cache,
+            bank_forks_arc,
+            sender,
+        );
         let chunks = create_duplicate_proof(
             my_keypair.clone(),
             None,
-            1,
+            start_slot,
             None,
             DUPLICATE_SHRED_MAX_PAYLOAD_SIZE,
         )
@@ -288,20 +332,24 @@ mod tests {
         let chunks1 = create_duplicate_proof(
             my_keypair.clone(),
             None,
-            2,
+            start_slot + 1,
             None,
             DUPLICATE_SHRED_MAX_PAYLOAD_SIZE,
         )
         .unwrap();
-        assert!(!blockstore.has_duplicate_shreds_in_slot(1));
-        assert!(!blockstore.has_duplicate_shreds_in_slot(2));
+        assert!(!blockstore.has_duplicate_shreds_in_slot(start_slot));
+        assert!(!blockstore.has_duplicate_shreds_in_slot(start_slot + 1));
         // Test that two proofs are mixed together, but we can store the proofs fine.
         for (chunk1, chunk2) in chunks.zip(chunks1) {
             duplicate_shred_handler.handle(chunk1);
             duplicate_shred_handler.handle(chunk2);
         }
-        assert!(blockstore.has_duplicate_shreds_in_slot(1));
-        assert!(blockstore.has_duplicate_shreds_in_slot(2));
+        assert!(blockstore.has_duplicate_shreds_in_slot(start_slot));
+        assert!(blockstore.has_duplicate_shreds_in_slot(start_slot + 1));
+        assert_eq!(
+            receiver.try_iter().collect_vec(),
+            vec![start_slot, start_slot + 1]
+        );
 
         // Test all kinds of bad proofs.
         for error in [
@@ -312,7 +360,7 @@ mod tests {
             match create_duplicate_proof(
                 my_keypair.clone(),
                 None,
-                3,
+                start_slot + 2,
                 Some(error),
                 DUPLICATE_SHRED_MAX_PAYLOAD_SIZE,
             ) {
@@ -321,7 +369,8 @@ mod tests {
                     for chunk in chunks {
                         duplicate_shred_handler.handle(chunk);
                     }
-                    assert!(!blockstore.has_duplicate_shreds_in_slot(3));
+                    assert!(!blockstore.has_duplicate_shreds_in_slot(start_slot + 2));
+                    assert!(receiver.is_empty());
                 }
             }
         }
@@ -337,13 +386,29 @@ mod tests {
         let my_pubkey = my_keypair.pubkey();
         let genesis_config_info = create_genesis_config_with_leader(10_000, &my_pubkey, 10_000);
         let GenesisConfigInfo { genesis_config, .. } = genesis_config_info;
-        let bank_forks = BankForks::new_rw_arc(Bank::new_for_tests(&genesis_config));
+        let mut bank = Bank::new_for_tests(&genesis_config);
+        bank.activate_feature(&feature_set::enable_gossip_duplicate_proof_ingestion::id());
+        let slots_in_epoch = bank.get_epoch_info().slots_in_epoch;
+        let bank_forks_arc = BankForks::new_rw_arc(bank);
+        {
+            let mut bank_forks = bank_forks_arc.write().unwrap();
+            let bank0 = bank_forks.get(0).unwrap();
+            bank_forks.insert(Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 9));
+            bank_forks.set_root(9, &AbsRequestSender::default(), None);
+        }
+        blockstore.set_roots([0, 9].iter()).unwrap();
         let leader_schedule_cache = Arc::new(LeaderScheduleCache::new_from_bank(
-            &bank_forks.read().unwrap().working_bank(),
+            &bank_forks_arc.read().unwrap().working_bank(),
         ));
-        let mut duplicate_shred_handler =
-            DuplicateShredHandler::new(blockstore.clone(), leader_schedule_cache, bank_forks);
-        let start_slot: Slot = 1;
+        let (sender, receiver) = unbounded();
+        let mut duplicate_shred_handler = DuplicateShredHandler::new(
+            blockstore.clone(),
+            leader_schedule_cache,
+            bank_forks_arc,
+            sender,
+        );
+        // The feature will only be activated at Epoch 1.
+        let start_slot: Slot = slots_in_epoch + 1;
 
         // This proof will not be accepted because num_chunks is too large.
         let chunks = create_duplicate_proof(
@@ -358,10 +423,11 @@ mod tests {
             duplicate_shred_handler.handle(chunk);
         }
         assert!(!blockstore.has_duplicate_shreds_in_slot(start_slot));
+        assert!(receiver.is_empty());
 
         // This proof will be rejected because the slot is too far away in the future.
         let future_slot =
-            blockstore.last_root() + duplicate_shred_handler.cached_slots_in_epoch + start_slot;
+            blockstore.max_root() + duplicate_shred_handler.cached_slots_in_epoch + start_slot;
         let chunks = create_duplicate_proof(
             my_keypair.clone(),
             None,
@@ -374,6 +440,7 @@ mod tests {
             duplicate_shred_handler.handle(chunk);
         }
         assert!(!blockstore.has_duplicate_shreds_in_slot(future_slot));
+        assert!(receiver.is_empty());
 
         // Send in two proofs, the first proof showing up will be accepted, the following
         // proofs will be discarded.
@@ -388,10 +455,54 @@ mod tests {
         // handle chunk 0 of the first proof.
         duplicate_shred_handler.handle(chunks.next().unwrap());
         assert!(!blockstore.has_duplicate_shreds_in_slot(start_slot));
+        assert!(receiver.is_empty());
         // Now send in the rest of the first proof, it will succeed.
         for chunk in chunks {
             duplicate_shred_handler.handle(chunk);
         }
         assert!(blockstore.has_duplicate_shreds_in_slot(start_slot));
+        assert_eq!(receiver.try_iter().collect_vec(), vec![start_slot]);
+    }
+
+    #[test]
+    fn test_feature_disabled() {
+        let ledger_path = get_tmp_ledger_path_auto_delete!();
+        let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
+        let my_keypair = Arc::new(Keypair::new());
+        let my_pubkey = my_keypair.pubkey();
+        let genesis_config_info = create_genesis_config_with_leader(10_000, &my_pubkey, 10_000);
+        let GenesisConfigInfo { genesis_config, .. } = genesis_config_info;
+        let mut bank = Bank::new_for_tests(&genesis_config);
+        bank.deactivate_feature(&feature_set::enable_gossip_duplicate_proof_ingestion::id());
+        assert!(!bank
+            .feature_set
+            .is_active(&feature_set::enable_gossip_duplicate_proof_ingestion::id()));
+        let bank_forks_arc = BankForks::new_rw_arc(bank);
+        let leader_schedule_cache = Arc::new(LeaderScheduleCache::new_from_bank(
+            &bank_forks_arc.read().unwrap().working_bank(),
+        ));
+        let (sender, receiver) = unbounded();
+
+        let mut duplicate_shred_handler = DuplicateShredHandler::new(
+            blockstore.clone(),
+            leader_schedule_cache,
+            bank_forks_arc,
+            sender,
+        );
+        let chunks = create_duplicate_proof(
+            my_keypair.clone(),
+            None,
+            1,
+            None,
+            DUPLICATE_SHRED_MAX_PAYLOAD_SIZE,
+        )
+        .unwrap();
+        assert!(!blockstore.has_duplicate_shreds_in_slot(1));
+        for chunk in chunks {
+            duplicate_shred_handler.handle(chunk);
+        }
+        // If feature disabled, blockstore gets signal but state machine doesn't see it.
+        assert!(blockstore.has_duplicate_shreds_in_slot(1));
+        assert!(receiver.try_iter().collect_vec().is_empty());
     }
 }
diff --git a/gossip/src/epoch_slots.rs b/gossip/src/epoch_slots.rs
index dc94380b33..c589e34814 100644
--- a/gossip/src/epoch_slots.rs
+++ b/gossip/src/epoch_slots.rs
@@ -13,7 +13,7 @@ use {
     },
 };
 
-const MAX_SLOTS_PER_ENTRY: usize = 2048 * 8;
+pub const MAX_SLOTS_PER_ENTRY: usize = 2048 * 8;
 #[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, AbiExample)]
 pub struct Uncompressed {
     pub first_slot: Slot,
@@ -178,7 +178,7 @@ impl Default for CompressedSlots {
 }
 
 impl CompressedSlots {
-    fn new(max_size: usize) -> Self {
+    pub(crate) fn new(max_size: usize) -> Self {
         CompressedSlots::Uncompressed(Uncompressed::new(max_size))
     }
 
diff --git a/gossip/src/gossip_service.rs b/gossip/src/gossip_service.rs
index 03ade4e709..b4a2403325 100644
--- a/gossip/src/gossip_service.rs
+++ b/gossip/src/gossip_service.rs
@@ -58,7 +58,6 @@ impl GossipService {
             Duration::from_millis(1), // coalesce
             false,
             None,
-            false,
         );
         let (consume_sender, listen_receiver) = unbounded();
         let t_socket_consume = cluster_info.clone().start_socket_consume_thread(
@@ -194,6 +193,7 @@ pub fn discover(
 }
 
 /// Creates a ThinClient by selecting a valid node at random
+#[deprecated(since = "1.18.6", note = "Interface will change")]
 pub fn get_client(
     nodes: &[ContactInfo],
     socket_addr_space: &SocketAddrSpace,
@@ -209,6 +209,7 @@ pub fn get_client(
     ThinClient::new(rpc, tpu, connection_cache)
 }
 
+#[deprecated(since = "1.18.6", note = "Will be removed in favor of get_client")]
 pub fn get_multi_client(
     nodes: &[ContactInfo],
     socket_addr_space: &SocketAddrSpace,
diff --git a/gossip/src/lib.rs b/gossip/src/lib.rs
index 11b609f3a3..2aea3078bb 100644
--- a/gossip/src/lib.rs
+++ b/gossip/src/lib.rs
@@ -24,6 +24,7 @@ pub mod legacy_contact_info;
 pub mod ping_pong;
 mod push_active_set;
 mod received_cache;
+pub mod restart_crds_values;
 pub mod weighted_shuffle;
 
 #[macro_use]
diff --git a/gossip/src/push_active_set.rs b/gossip/src/push_active_set.rs
index 8b6dcb6f58..1e7e3cbb22 100644
--- a/gossip/src/push_active_set.rs
+++ b/gossip/src/push_active_set.rs
@@ -2,7 +2,7 @@ use {
     crate::weighted_shuffle::WeightedShuffle,
     indexmap::IndexMap,
     rand::Rng,
-    solana_bloom::bloom::{AtomicBloom, Bloom},
+    solana_bloom::bloom::{Bloom, ConcurrentBloom},
     solana_sdk::{native_token::LAMPORTS_PER_SOL, pubkey::Pubkey},
     std::collections::HashMap,
 };
@@ -19,7 +19,7 @@ pub(crate) struct PushActiveSet([PushActiveSetEntry; NUM_PUSH_ACTIVE_SET_ENTRIES
 // Keys are gossip nodes to push messages to.
 // Values are which origins the node has pruned.
 #[derive(Default)]
-struct PushActiveSetEntry(IndexMap</*node:*/ Pubkey, /*origins:*/ AtomicBloom<Pubkey>>);
+struct PushActiveSetEntry(IndexMap</*node:*/ Pubkey, /*origins:*/ ConcurrentBloom<Pubkey>>);
 
 impl PushActiveSet {
     #[cfg(debug_assertions)]
@@ -151,7 +151,7 @@ impl PushActiveSetEntry {
             if self.0.contains_key(node) {
                 continue;
             }
-            let bloom = AtomicBloom::from(Bloom::random(
+            let bloom = ConcurrentBloom::from(Bloom::random(
                 num_bloom_filter_items,
                 Self::BLOOM_FALSE_RATE,
                 Self::BLOOM_MAX_BITS,
diff --git a/gossip/src/restart_crds_values.rs b/gossip/src/restart_crds_values.rs
new file mode 100644
index 0000000000..4a2606335b
--- /dev/null
+++ b/gossip/src/restart_crds_values.rs
@@ -0,0 +1,370 @@
+use {
+    crate::crds_value::{new_rand_timestamp, sanitize_wallclock},
+    bv::BitVec,
+    itertools::Itertools,
+    rand::Rng,
+    solana_sdk::{
+        clock::Slot,
+        hash::Hash,
+        pubkey::Pubkey,
+        sanitize::{Sanitize, SanitizeError},
+        serde_varint,
+    },
+    thiserror::Error,
+};
+
+#[derive(Serialize, Deserialize, Clone, PartialEq, Eq, AbiExample, Debug)]
+pub struct RestartLastVotedForkSlots {
+    pub from: Pubkey,
+    pub wallclock: u64,
+    offsets: SlotsOffsets,
+    pub last_voted_slot: Slot,
+    pub last_voted_hash: Hash,
+    pub shred_version: u16,
+}
+
+#[derive(Debug, Error)]
+pub enum RestartLastVotedForkSlotsError {
+    #[error("Last voted fork cannot be empty")]
+    LastVotedForkEmpty,
+}
+
+#[derive(Serialize, Deserialize, Clone, PartialEq, Eq, AbiExample, Debug)]
+pub struct RestartHeaviestFork {
+    pub from: Pubkey,
+    pub wallclock: u64,
+    pub last_slot: Slot,
+    pub last_slot_hash: Hash,
+    pub observed_stake: u64,
+    pub shred_version: u16,
+}
+
+#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, AbiExample, AbiEnumVisitor)]
+enum SlotsOffsets {
+    RunLengthEncoding(RunLengthEncoding),
+    RawOffsets(RawOffsets),
+}
+
+#[derive(Deserialize, Serialize, Clone, Debug, PartialEq, Eq, AbiExample)]
+struct U16(#[serde(with = "serde_varint")] u16);
+
+// The vector always starts with 1. Encode number of 1's and 0's consecutively.
+// For example, 110000111 is [2, 4, 3].
+#[derive(Deserialize, Serialize, Clone, Debug, PartialEq, Eq, AbiExample)]
+struct RunLengthEncoding(Vec<U16>);
+
+#[derive(Deserialize, Serialize, Clone, Debug, PartialEq, Eq, AbiExample)]
+struct RawOffsets(BitVec<u8>);
+
+impl Sanitize for RestartLastVotedForkSlots {
+    fn sanitize(&self) -> std::result::Result<(), SanitizeError> {
+        sanitize_wallclock(self.wallclock)?;
+        self.last_voted_hash.sanitize()
+    }
+}
+
+impl RestartLastVotedForkSlots {
+    // This number is MAX_CRDS_OBJECT_SIZE - empty serialized RestartLastVotedForkSlots.
+    const MAX_BYTES: usize = 824;
+
+    // Per design doc, we should start wen_restart within 7 hours.
+    pub const MAX_SLOTS: usize = u16::MAX as usize;
+
+    pub fn new(
+        from: Pubkey,
+        now: u64,
+        last_voted_fork: &[Slot],
+        last_voted_hash: Hash,
+        shred_version: u16,
+    ) -> Result<Self, RestartLastVotedForkSlotsError> {
+        let Some((&first_voted_slot, &last_voted_slot)) =
+            last_voted_fork.iter().minmax().into_option()
+        else {
+            return Err(RestartLastVotedForkSlotsError::LastVotedForkEmpty);
+        };
+        let max_size = last_voted_slot.saturating_sub(first_voted_slot) + 1;
+        let mut uncompressed_bitvec = BitVec::new_fill(false, max_size);
+        for slot in last_voted_fork {
+            uncompressed_bitvec.set(last_voted_slot - *slot, true);
+        }
+        let run_length_encoding = RunLengthEncoding::new(&uncompressed_bitvec);
+        let offsets =
+            if run_length_encoding.num_encoded_slots() > RestartLastVotedForkSlots::MAX_BYTES * 8 {
+                SlotsOffsets::RunLengthEncoding(run_length_encoding)
+            } else {
+                SlotsOffsets::RawOffsets(RawOffsets::new(uncompressed_bitvec))
+            };
+        Ok(Self {
+            from,
+            wallclock: now,
+            offsets,
+            last_voted_slot,
+            last_voted_hash,
+            shred_version,
+        })
+    }
+
+    /// New random Version for tests and benchmarks.
+    pub(crate) fn new_rand<R: Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> Self {
+        let pubkey = pubkey.unwrap_or_else(solana_sdk::pubkey::new_rand);
+        let num_slots = rng.gen_range(2..20);
+        let slots = std::iter::repeat_with(|| 47825632 + rng.gen_range(0..512))
+            .take(num_slots)
+            .collect::<Vec<Slot>>();
+        RestartLastVotedForkSlots::new(
+            pubkey,
+            new_rand_timestamp(rng),
+            &slots,
+            Hash::new_unique(),
+            1,
+        )
+        .unwrap()
+    }
+
+    pub fn to_slots(&self, min_slot: Slot) -> Vec<Slot> {
+        match &self.offsets {
+            SlotsOffsets::RunLengthEncoding(run_length_encoding) => {
+                run_length_encoding.to_slots(self.last_voted_slot, min_slot)
+            }
+            SlotsOffsets::RawOffsets(raw_offsets) => {
+                raw_offsets.to_slots(self.last_voted_slot, min_slot)
+            }
+        }
+    }
+}
+
+impl Sanitize for RestartHeaviestFork {
+    fn sanitize(&self) -> Result<(), SanitizeError> {
+        sanitize_wallclock(self.wallclock)?;
+        self.last_slot_hash.sanitize()
+    }
+}
+
+impl RestartHeaviestFork {
+    pub(crate) fn new_rand<R: Rng>(rng: &mut R, from: Option<Pubkey>) -> Self {
+        let from = from.unwrap_or_else(solana_sdk::pubkey::new_rand);
+        Self {
+            from,
+            wallclock: new_rand_timestamp(rng),
+            last_slot: rng.gen_range(0..1000),
+            last_slot_hash: Hash::new_unique(),
+            observed_stake: rng.gen_range(1..u64::MAX),
+            shred_version: 1,
+        }
+    }
+}
+
+impl RunLengthEncoding {
+    fn new(bits: &BitVec<u8>) -> Self {
+        let encoded = (0..bits.len())
+            .map(|i| bits.get(i))
+            .dedup_with_count()
+            .map_while(|(count, _)| u16::try_from(count).ok())
+            .scan(0, |current_bytes, count| {
+                *current_bytes += ((u16::BITS - count.leading_zeros() + 6) / 7).max(1) as usize;
+                (*current_bytes <= RestartLastVotedForkSlots::MAX_BYTES).then_some(U16(count))
+            })
+            .collect();
+        Self(encoded)
+    }
+
+    fn num_encoded_slots(&self) -> usize {
+        self.0.iter().map(|x| usize::from(x.0)).sum()
+    }
+
+    fn to_slots(&self, last_slot: Slot, min_slot: Slot) -> Vec<Slot> {
+        let mut slots: Vec<Slot> = self
+            .0
+            .iter()
+            .map(|bit_count| usize::from(bit_count.0))
+            .zip([1, 0].iter().cycle())
+            .flat_map(|(bit_count, bit)| std::iter::repeat(bit).take(bit_count))
+            .enumerate()
+            .filter(|(_, bit)| **bit == 1)
+            .map_while(|(offset, _)| {
+                let offset = Slot::try_from(offset).ok()?;
+                last_slot.checked_sub(offset)
+            })
+            .take(RestartLastVotedForkSlots::MAX_SLOTS)
+            .take_while(|slot| *slot >= min_slot)
+            .collect();
+        slots.reverse();
+        slots
+    }
+}
+
+impl RawOffsets {
+    fn new(mut bits: BitVec<u8>) -> Self {
+        bits.truncate(RestartLastVotedForkSlots::MAX_BYTES as u64 * 8);
+        bits.shrink_to_fit();
+        Self(bits)
+    }
+
+    fn to_slots(&self, last_slot: Slot, min_slot: Slot) -> Vec<Slot> {
+        let mut slots: Vec<Slot> = (0..self.0.len())
+            .filter(|index| self.0.get(*index))
+            .map_while(|offset| last_slot.checked_sub(offset))
+            .take_while(|slot| *slot >= min_slot)
+            .collect();
+        slots.reverse();
+        slots
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use {
+        super::*,
+        crate::{
+            cluster_info::MAX_CRDS_OBJECT_SIZE,
+            crds_value::{CrdsData, CrdsValue, CrdsValueLabel},
+        },
+        bincode::serialized_size,
+        solana_sdk::{signature::Signer, signer::keypair::Keypair, timing::timestamp},
+        std::iter::repeat_with,
+    };
+
+    fn make_rand_slots<R: Rng>(rng: &mut R) -> impl Iterator<Item = Slot> + '_ {
+        repeat_with(|| rng.gen_range(1..5)).scan(0, |slot, step| {
+            *slot += step;
+            Some(*slot)
+        })
+    }
+
+    #[test]
+    fn test_restart_last_voted_fork_slots_max_bytes() {
+        let keypair = Keypair::new();
+        let header = RestartLastVotedForkSlots::new(
+            keypair.pubkey(),
+            timestamp(),
+            &[1, 2],
+            Hash::default(),
+            0,
+        )
+        .unwrap();
+        // If the following assert fails, please update RestartLastVotedForkSlots::MAX_BYTES
+        assert_eq!(
+            RestartLastVotedForkSlots::MAX_BYTES,
+            MAX_CRDS_OBJECT_SIZE - serialized_size(&header).unwrap() as usize
+        );
+
+        // Create large enough slots to make sure we are discarding some to make slots fit.
+        let mut rng = rand::thread_rng();
+        let large_length = 8000;
+        let range: Vec<Slot> = make_rand_slots(&mut rng).take(large_length).collect();
+        let large_slots = RestartLastVotedForkSlots::new(
+            keypair.pubkey(),
+            timestamp(),
+            &range,
+            Hash::default(),
+            0,
+        )
+        .unwrap();
+        assert!(serialized_size(&large_slots).unwrap() <= MAX_CRDS_OBJECT_SIZE as u64);
+        let retrieved_slots = large_slots.to_slots(0);
+        assert!(retrieved_slots.len() <= range.len());
+        assert!(retrieved_slots.last().unwrap() - retrieved_slots.first().unwrap() > 5000);
+    }
+
+    #[test]
+    fn test_restart_last_voted_fork_slots() {
+        let keypair = Keypair::new();
+        let slot = 53;
+        let slot_parent = slot - 5;
+        let shred_version = 21;
+        let original_slots_vec = [slot_parent, slot];
+        let slots = RestartLastVotedForkSlots::new(
+            keypair.pubkey(),
+            timestamp(),
+            &original_slots_vec,
+            Hash::default(),
+            shred_version,
+        )
+        .unwrap();
+        let value =
+            CrdsValue::new_signed(CrdsData::RestartLastVotedForkSlots(slots.clone()), &keypair);
+        assert_eq!(value.sanitize(), Ok(()));
+        let label = value.label();
+        assert_eq!(
+            label,
+            CrdsValueLabel::RestartLastVotedForkSlots(keypair.pubkey())
+        );
+        assert_eq!(label.pubkey(), keypair.pubkey());
+        assert_eq!(value.wallclock(), slots.wallclock);
+        let retrieved_slots = slots.to_slots(0);
+        assert_eq!(retrieved_slots.len(), 2);
+        assert_eq!(retrieved_slots[0], slot_parent);
+        assert_eq!(retrieved_slots[1], slot);
+
+        let bad_value = RestartLastVotedForkSlots::new(
+            keypair.pubkey(),
+            timestamp(),
+            &[],
+            Hash::default(),
+            shred_version,
+        );
+        assert!(bad_value.is_err());
+
+        let last_slot: Slot = 8000;
+        let large_slots_vec: Vec<Slot> = (0..last_slot + 1).collect();
+        let large_slots = RestartLastVotedForkSlots::new(
+            keypair.pubkey(),
+            timestamp(),
+            &large_slots_vec,
+            Hash::default(),
+            shred_version,
+        )
+        .unwrap();
+        assert!(serialized_size(&large_slots).unwrap() < MAX_CRDS_OBJECT_SIZE as u64);
+        let retrieved_slots = large_slots.to_slots(0);
+        assert_eq!(retrieved_slots, large_slots_vec);
+    }
+
+    fn check_run_length_encoding(slots: Vec<Slot>) {
+        let last_voted_slot = slots[slots.len() - 1];
+        let mut bitvec = BitVec::new_fill(false, last_voted_slot - slots[0] + 1);
+        for slot in &slots {
+            bitvec.set(last_voted_slot - slot, true);
+        }
+        let rle = RunLengthEncoding::new(&bitvec);
+        let retrieved_slots = rle.to_slots(last_voted_slot, 0);
+        assert_eq!(retrieved_slots, slots);
+    }
+
+    #[test]
+    fn test_run_length_encoding() {
+        check_run_length_encoding((1000..16384 + 1000).map(|x| x as Slot).collect_vec());
+        check_run_length_encoding([1000 as Slot].into());
+        check_run_length_encoding(
+            [
+                1000 as Slot,
+                RestartLastVotedForkSlots::MAX_SLOTS as Slot + 999,
+            ]
+            .into(),
+        );
+        check_run_length_encoding((1000..1800).step_by(2).map(|x| x as Slot).collect_vec());
+
+        let mut rng = rand::thread_rng();
+        let large_length = 500;
+        let range: Vec<Slot> = make_rand_slots(&mut rng).take(large_length).collect();
+        check_run_length_encoding(range);
+    }
+
+    #[test]
+    fn test_restart_heaviest_fork() {
+        let keypair = Keypair::new();
+        let slot = 53;
+        let mut fork = RestartHeaviestFork {
+            from: keypair.pubkey(),
+            wallclock: timestamp(),
+            last_slot: slot,
+            last_slot_hash: Hash::default(),
+            observed_stake: 800_000,
+            shred_version: 1,
+        };
+        assert_eq!(fork.sanitize(), Ok(()));
+        assert_eq!(fork.observed_stake, 800_000);
+        fork.wallclock = crate::crds_value::MAX_WALLCLOCK;
+        assert_eq!(fork.sanitize(), Err(SanitizeError::ValueOutOfBounds));
+    }
+}
diff --git a/gossip/src/weighted_shuffle.rs b/gossip/src/weighted_shuffle.rs
index 250d1efb0f..656615449b 100644
--- a/gossip/src/weighted_shuffle.rs
+++ b/gossip/src/weighted_shuffle.rs
@@ -9,6 +9,14 @@ use {
     std::ops::{AddAssign, Sub, SubAssign},
 };
 
+// Each internal tree node has FANOUT many child nodes with indices:
+//     (index << BIT_SHIFT) + 1 ..= (index << BIT_SHIFT) + FANOUT
+// Conversely, for each node, the parent node is obtained by:
+//     (index - 1) >> BIT_SHIFT
+const BIT_SHIFT: usize = 4;
+const FANOUT: usize = 1 << BIT_SHIFT;
+const BIT_MASK: usize = FANOUT - 1;
+
 /// Implements an iterator where indices are shuffled according to their
 /// weights:
 ///   - Returned indices are unique in the range [0, weights.len()).
@@ -18,15 +26,15 @@ use {
 ///     non-zero weighted indices.
 #[derive(Clone)]
 pub struct WeightedShuffle<T> {
-    arr: Vec<T>,       // Underlying array implementing binary indexed tree.
-    sum: T,            // Current sum of weights, excluding already selected indices.
-    zeros: Vec<usize>, // Indices of zero weighted entries.
+    // Underlying array implementing the tree.
+    // tree[i][j] is the sum of all weights in the j'th sub-tree of node i.
+    tree: Vec<[T; FANOUT - 1]>,
+    // Current sum of all weights, excluding already sampled ones.
+    weight: T,
+    // Indices of zero weighted entries.
+    zeros: Vec<usize>,
 }
 
-// The implementation uses binary indexed tree:
-// https://en.wikipedia.org/wiki/Fenwick_tree
-// to maintain cumulative sum of weights excluding already selected indices
-// over self.arr.
 impl<T> WeightedShuffle<T>
 where
     T: Copy + Default + PartialOrd + AddAssign + CheckedAdd,
@@ -34,36 +42,41 @@ where
     /// If weights are negative or overflow the total sum
     /// they are treated as zero.
     pub fn new(name: &'static str, weights: &[T]) -> Self {
-        let size = weights.len() + 1;
         let zero = <T as Default>::default();
-        let mut arr = vec![zero; size];
+        let mut tree = vec![[zero; FANOUT - 1]; get_tree_size(weights.len())];
         let mut sum = zero;
         let mut zeros = Vec::default();
         let mut num_negative = 0;
         let mut num_overflow = 0;
-        for (mut k, &weight) in (1usize..).zip(weights) {
+        for (k, &weight) in weights.iter().enumerate() {
             #[allow(clippy::neg_cmp_op_on_partial_ord)]
             // weight < zero does not work for NaNs.
             if !(weight >= zero) {
-                zeros.push(k - 1);
+                zeros.push(k);
                 num_negative += 1;
                 continue;
             }
             if weight == zero {
-                zeros.push(k - 1);
+                zeros.push(k);
                 continue;
             }
             sum = match sum.checked_add(&weight) {
                 Some(val) => val,
                 None => {
-                    zeros.push(k - 1);
+                    zeros.push(k);
                     num_overflow += 1;
                     continue;
                 }
             };
-            while k < size {
-                arr[k] += weight;
-                k += k & k.wrapping_neg();
+            // Traverse the tree from the leaf node upwards to the root,
+            // updating the sub-tree sums along the way.
+            let mut index = tree.len() + k; // leaf node
+            while index != 0 {
+                let offset = index & BIT_MASK;
+                index = (index - 1) >> BIT_SHIFT; // parent node
+                if offset > 0 {
+                    tree[index][offset - 1] += weight;
+                }
             }
         }
         if num_negative > 0 {
@@ -72,7 +85,11 @@ where
         if num_overflow > 0 {
             datapoint_error!("weighted-shuffle-overflow", (name, num_overflow, i64));
         }
-        Self { arr, sum, zeros }
+        Self {
+            tree,
+            weight: sum,
+            zeros,
+        }
     }
 }
 
@@ -80,54 +97,84 @@ impl<T> WeightedShuffle<T>
 where
     T: Copy + Default + PartialOrd + AddAssign + SubAssign + Sub<Output = T>,
 {
-    // Returns cumulative sum of current weights upto index k (inclusive).
-    fn cumsum(&self, mut k: usize) -> T {
-        let mut out = <T as Default>::default();
-        while k != 0 {
-            out += self.arr[k];
-            k ^= k & k.wrapping_neg();
-        }
-        out
-    }
-
     // Removes given weight at index k.
-    fn remove(&mut self, mut k: usize, weight: T) {
-        self.sum -= weight;
-        let size = self.arr.len();
-        while k < size {
-            self.arr[k] -= weight;
-            k += k & k.wrapping_neg();
+    fn remove(&mut self, k: usize, weight: T) {
+        debug_assert!(self.weight >= weight);
+        self.weight -= weight;
+        // Traverse the tree from the leaf node upwards to the root,
+        // updating the sub-tree sums along the way.
+        let mut index = self.tree.len() + k; // leaf node
+        while index != 0 {
+            let offset = index & BIT_MASK;
+            index = (index - 1) >> BIT_SHIFT; // parent node
+            if offset > 0 {
+                debug_assert!(self.tree[index][offset - 1] >= weight);
+                self.tree[index][offset - 1] -= weight;
+            }
         }
     }
 
-    // Returns smallest index such that self.cumsum(k) > val,
+    // Returns smallest index such that sum of weights[..=k] > val,
     // along with its respective weight.
-    fn search(&self, val: T) -> (/*index:*/ usize, /*weight:*/ T) {
+    fn search(&self, mut val: T) -> (/*index:*/ usize, /*weight:*/ T) {
         let zero = <T as Default>::default();
         debug_assert!(val >= zero);
-        debug_assert!(val < self.sum);
-        let mut lo = (/*index:*/ 0, /*cumsum:*/ zero);
-        let mut hi = (self.arr.len() - 1, self.sum);
-        while lo.0 + 1 < hi.0 {
-            let k = lo.0 + (hi.0 - lo.0) / 2;
-            let sum = self.cumsum(k);
-            if sum <= val {
-                lo = (k, sum);
-            } else {
-                hi = (k, sum);
+        debug_assert!(val < self.weight);
+        // Traverse the tree downwards from the root while maintaining the
+        // weight of the subtree which contains the target leaf node.
+        let mut index = 0; // root
+        let mut weight = self.weight;
+        'outer: while index < self.tree.len() {
+            for (j, &node) in self.tree[index].iter().enumerate() {
+                if val < node {
+                    // Traverse to the j+1 subtree of self.tree[index].
+                    weight = node;
+                    index = (index << BIT_SHIFT) + j + 1;
+                    continue 'outer;
+                } else {
+                    debug_assert!(weight >= node);
+                    weight -= node;
+                    val -= node;
+                }
             }
+            // Traverse to the right-most subtree of self.tree[index].
+            index = (index << BIT_SHIFT) + FANOUT;
         }
-        debug_assert!(lo.1 <= val);
-        debug_assert!(hi.1 > val);
-        (hi.0, hi.1 - lo.1)
+        (index - self.tree.len(), weight)
     }
 
-    pub fn remove_index(&mut self, index: usize) {
-        let zero = <T as Default>::default();
-        let weight = self.cumsum(index + 1) - self.cumsum(index);
-        if weight != zero {
-            self.remove(index + 1, weight);
-        } else if let Some(index) = self.zeros.iter().position(|ix| *ix == index) {
+    pub fn remove_index(&mut self, k: usize) {
+        // Traverse the tree from the leaf node upwards to the root, while
+        // maintaining the sum of weights of subtrees *not* containing the leaf
+        // node.
+        let mut index = self.tree.len() + k; // leaf node
+        let mut weight = <T as Default>::default(); // zero
+        while index != 0 {
+            let offset = index & BIT_MASK;
+            index = (index - 1) >> BIT_SHIFT; // parent node
+            if offset > 0 {
+                if self.tree[index][offset - 1] != weight {
+                    self.remove(k, self.tree[index][offset - 1] - weight);
+                } else {
+                    self.remove_zero(k);
+                }
+                return;
+            }
+            // The leaf node is in the right-most subtree of self.tree[index].
+            for &node in &self.tree[index] {
+                weight += node;
+            }
+        }
+        // The leaf node is the right-most node of the whole tree.
+        if self.weight != weight {
+            self.remove(k, self.weight - weight);
+        } else {
+            self.remove_zero(k);
+        }
+    }
+
+    fn remove_zero(&mut self, k: usize) {
+        if let Some(index) = self.zeros.iter().position(|&ix| ix == k) {
             self.zeros.remove(index);
         }
     }
@@ -140,10 +187,10 @@ where
     // Equivalent to weighted_shuffle.shuffle(&mut rng).next()
     pub fn first<R: Rng>(&self, rng: &mut R) -> Option<usize> {
         let zero = <T as Default>::default();
-        if self.sum > zero {
-            let sample = <T as SampleUniform>::Sampler::sample_single(zero, self.sum, rng);
+        if self.weight > zero {
+            let sample = <T as SampleUniform>::Sampler::sample_single(zero, self.weight, rng);
             let (index, _weight) = WeightedShuffle::search(self, sample);
-            return Some(index - 1);
+            return Some(index);
         }
         if self.zeros.is_empty() {
             return None;
@@ -160,11 +207,11 @@ where
     pub fn shuffle<R: Rng>(mut self, rng: &'a mut R) -> impl Iterator<Item = usize> + 'a {
         std::iter::from_fn(move || {
             let zero = <T as Default>::default();
-            if self.sum > zero {
-                let sample = <T as SampleUniform>::Sampler::sample_single(zero, self.sum, rng);
+            if self.weight > zero {
+                let sample = <T as SampleUniform>::Sampler::sample_single(zero, self.weight, rng);
                 let (index, weight) = WeightedShuffle::search(&self, sample);
                 self.remove(index, weight);
-                return Some(index - 1);
+                return Some(index);
             }
             if self.zeros.is_empty() {
                 return None;
@@ -176,6 +223,18 @@ where
     }
 }
 
+// Maps number of items to the "internal" size of the tree
+// which "implicitly" holds those items on the leaves.
+fn get_tree_size(count: usize) -> usize {
+    let mut size = if count == 1 { 1 } else { 0 };
+    let mut nodes = 1;
+    while nodes < count {
+        size += nodes;
+        nodes *= FANOUT;
+    }
+    size
+}
+
 #[cfg(test)]
 mod tests {
     use {
@@ -218,6 +277,23 @@ mod tests {
         shuffle
     }
 
+    #[test]
+    fn test_get_tree_size() {
+        assert_eq!(get_tree_size(0), 0);
+        for count in 1..=16 {
+            assert_eq!(get_tree_size(count), 1);
+        }
+        for count in 17..=256 {
+            assert_eq!(get_tree_size(count), 1 + 16);
+        }
+        for count in 257..=4096 {
+            assert_eq!(get_tree_size(count), 1 + 16 + 16 * 16);
+        }
+        for count in 4097..=65536 {
+            assert_eq!(get_tree_size(count), 1 + 16 + 16 * 16 + 16 * 16 * 16);
+        }
+    }
+
     // Asserts that empty weights will return empty shuffle.
     #[test]
     fn test_weighted_shuffle_empty_weights() {
@@ -357,4 +433,20 @@ mod tests {
             assert_eq!(shuffle.first(&mut rng), Some(shuffle_slow[0]));
         }
     }
+
+    #[test]
+    fn test_weighted_shuffle_paranoid() {
+        let mut rng = rand::thread_rng();
+        for size in 0..1351 {
+            let weights: Vec<_> = repeat_with(|| rng.gen_range(0..1000)).take(size).collect();
+            let seed = rng.gen::<[u8; 32]>();
+            let mut rng = ChaChaRng::from_seed(seed);
+            let shuffle_slow = weighted_shuffle_slow(&mut rng.clone(), weights.clone());
+            let shuffle = WeightedShuffle::new("", &weights);
+            if size > 0 {
+                assert_eq!(shuffle.first(&mut rng.clone()), Some(shuffle_slow[0]));
+            }
+            assert_eq!(shuffle.shuffle(&mut rng).collect::<Vec<_>>(), shuffle_slow);
+        }
+    }
 }
diff --git a/gossip/tests/crds_gossip.rs b/gossip/tests/crds_gossip.rs
index 74415ec3c8..ff9e36ba2c 100644
--- a/gossip/tests/crds_gossip.rs
+++ b/gossip/tests/crds_gossip.rs
@@ -351,9 +351,7 @@ fn network_run_push(
                 node.gossip.purge(&node_pubkey, thread_pool, now, &timeouts);
                 (
                     node_pubkey,
-                    node.gossip
-                        .new_push_messages(&node_pubkey, vec![], now, &stakes)
-                        .0,
+                    node.gossip.new_push_messages(&node_pubkey, now, &stakes).0,
                 )
             })
             .collect();
diff --git a/gossip/tests/gossip.rs b/gossip/tests/gossip.rs
index 569f7c480d..7759679bdf 100644
--- a/gossip/tests/gossip.rs
+++ b/gossip/tests/gossip.rs
@@ -139,14 +139,16 @@ fn retransmit_to(
             .filter(|addr| socket_addr_space.check(addr))
             .collect()
     };
-    if let Err(SendPktsError::IoError(ioerr, num_failed)) = multi_target_send(socket, data, &dests)
-    {
-        error!(
-            "retransmit_to multi_target_send error: {:?}, {}/{} packets failed",
-            ioerr,
-            num_failed,
-            dests.len(),
-        );
+    match multi_target_send(socket, data, &dests) {
+        Ok(()) => (),
+        Err(SendPktsError::IoError(ioerr, num_failed)) => {
+            error!(
+                "retransmit_to multi_target_send error: {:?}, {}/{} packets failed",
+                ioerr,
+                num_failed,
+                dests.len(),
+            );
+        }
     }
 }
 
